---
title: Mechanics
author: David Kane and Anish Talla
tutorial:
  id: mechanics
output:
  learnr::tutorial:
    progressive: yes
    allow_skip:: yes
runtime: shiny_prerendered
description: 'Chapter 7 Tutorial: Mechanics'
---

```{r setup, include = FALSE}
library(learnr)
library(tutorial.helpers)
library(tidyverse)
library(primer.data)
library(broom.mixed)
library(patchwork)
library(tidymodels)

library(marginaleffects)
library(easystats)

knitr::opts_chunk$set(echo = FALSE)
options(tutorial.exercise.timelimit = 600, 
        tutorial.storage = "local") 

x <- kenya |> 
  filter(rv13 > 0)

rv_p <- x |> 
  ggplot(aes(rv13)) + 
    geom_histogram(bins = 100) +
    labs(x = "Registered Voters",
         y = NULL) 

log_rv_p <- x |> 
  ggplot(aes(log(rv13))) + 
    geom_histogram(bins = 100) +
    labs(x = "Log of Registered Voters",
         y = NULL) +
    expand_limits(y = c(0, 175))


# DK: Delete this garbage when you can or, rather, replace 

# Create and save all the models we use.

fit_1 <- linear_reg() |>
  fit(income ~ age, data = trains)

trains_2 <- trains |> 
  mutate(c_age = age - mean(age))

fit_1_c <- linear_reg() |>
  fit(income ~ c_age, data = trains_2)

trains_3 <- trains |> 
  mutate(s_age = age / sd(age))

fit_1_s <- linear_reg() |>
  fit(income ~ s_age, data = trains_3)

trains_4 <- trains |> 
  mutate(z_age = scale(age))

fit_1_z <- linear_reg() |>
  fit(income ~ z_age, data = trains_4)

no_na_nhanes <- nhanes |> 
  select(height, age) |> 
  drop_na() 

nhanes_1 <- linear_reg() |>
  fit(height ~ age, data = no_na_nhanes)

nhanes_2 <- linear_reg() |>
  fit(height ~ age + I(age^2), data = no_na_nhanes)

nhanes_3 <- linear_reg() |>
  fit(height ~ I(ifelse(age > 18, 18, age)), data = no_na_nhanes)

fit_1_model <- linear_reg() |>
  fit(att_end ~ treatment + att_start, data = trains)

fit_2_model <- linear_reg() |>
  fit(income ~ age + liberal, data = trains)

fit_liberal <- linear_reg() |> 
  fit(att_end ~ liberal, data = trains)

# DK: use compare_performance() from easystats to compare
# models. 

performance_liberal <- performance(fit_liberal)

fit_att_start <- linear_reg() |>
  fit(att_end ~ att_start, data = trains)

performance_att_start <- performance(fit_att_start)

compare <- compare_performance(fit_liberal, fit_att_start) 

```

```{r copy-code-chunk, child = system.file("child_documents/copy_button.Rmd", package = "tutorial.helpers")}
```

```{r info-section, child = system.file("child_documents/info_section.Rmd", package = "tutorial.helpers")}
```

<!-- DK: Replace brms with tidymodels. Done -->
<!-- DK: Use broom not broom.mixed. Done-->
<!-- DK: Add test code chunks for anything not too slow. -->
<!-- DK: Delete all data/*rds which are no longer used. Done -->

## Introduction
### 

This tutorial supports [*Preceptor’s Primer for Bayesian Data Science: Using the Cardinal Virtues for Inference*](https://ppbds.github.io/primer/) by [David Kane](https://davidkane.info/). 

The world confronts us. Make decisions we must.

In our haste to make progress --- to get all the way through the process of building, interpreting and using models --- we have given short shrift to some of the messy details of model building and evaluation. This chapter fills in those lacunae.

## Transforming variables 
### 

It is often convenient to transform a predictor variable so that our model makes more sense.

### 

Let's say we are interested in predicting a person's income based on their age. We have a model of `income` as a function of `age`: 

$$ income_i = \beta_0  + \beta_1 age_i + \epsilon_i$$

### Exercise 1

Load the **tidymodels** package.

```{r transforming-variables-1, exercise = TRUE}

```

```{r transforming-variables-1-hint-1, eval = FALSE}
library(tidymodels)
```

```{r transforming-variables-1-test, include = FALSE}
library(tidymodels)
```

### 

Since `income` is a continuous variable, we use the **tidymodels** framework to build linear regression models. A model that establishes the relationship between two variables is called a **simple regression** model.

### Exercise 2

Load the **tidyverse** package.

```{r transforming-variables-2, exercise = TRUE}

```

```{r transforming-variables-2-hint-1, eval = FALSE}
library(tidyverse)
```

```{r transforming-variables-2-test, include = FALSE}
library(tidyverse)
```

### 

The general form of the **simple regression** model is: 

$$ Y = \beta_0  + \beta_1 X + \epsilon_i$$

The model is **simple** not only because there is only one variable in the right side of the equation, but also because $X$ is not an exponential variable and thus the relationship between $X$ and $Y$ is constant, or linear. Thus, we can also call it **linear regression**.

### Exercise 3

We will be using the `trains` data set from **primer.data** for our model. Load the **primer.data** package.

```{r transforming-variables-3, exercise = TRUE}

```

```{r transforming-variables-3-hint-1, eval = FALSE}
library(primer.data)
```

```{r transforming-variables-3-test, include = FALSE}
library(primer.data)
```

### 

$\beta_0$ and $\beta_1$ are the coefficients, when we interpret coefficients, it is important to know the difference between *across unit* and *within unit comparisons*. The former refers to comparing two different units but not looking at causal relationship while the latter looks at that one unit under treatment versus when under control.

### Exercise 4

We will also be using the **easystats** package collection, which provides tools for model checking, performance, and visualization. Load the **easystats** package.

```{r transforming-variables-4, exercise = TRUE}

```

```{r transforming-variables-4-hint-1, eval = FALSE}
library(easystats)
```

```{r transforming-variables-4-test, include = FALSE}
library(easystats)
```

### 

The **easystats** collection offers tools for checking, comparing, and visualizing models. It simplifies extracting and tidying model results, helping you understand and communicate your analyses easily.

### Exercise 5

We will also be using a new package, **broom**, which allows us to tidy regression data for plotting. Load the **broom** package.

```{r transforming-variables-5, exercise = TRUE}

```

```{r transforming-variables-5-hint-1, eval = FALSE}
library(broom)
```

```{r transforming-variables-5-test, include = FALSE}
library(broom)
```

### 

In terms of the Preceptor Tables, *within unit comparison* is looking at one **row** of data, we are comparing **one unit**, or (in this case) one person, to itself under two conditions (control versus treatment). Meanwhile, *across unit comparison* is looking at multiple rows of data, with a focus on differences across columns, without making any causal claims.

### Exercise 6

Type `trains` and hit "Run Code".

```{r transforming-variables-6, exercise = TRUE}

```

```{r transforming-variables-6-hint-1, eval = FALSE}
trains
```

```{r transforming-variables-6-test, include = FALSE}
trains
```

### 

The model we are using only takes one variable, `age`, on the right hand side to predict `income`. However, are there any other variables in the data set that can also predict `income`, or potentially affect the relationship between `age` and `income`? We will discuss it in the following sections.

### Exercise 7

Pipe `linear_reg()` to `fit()`, with the formula `income ~ age` and `data = trains` as arguments.

<!-- DK: Fix all the instructions to two steps. Run this code. And then, "Behind the scenes." -->

```{r transforming-variables-7, exercise = TRUE}

```

```{r transforming-variables-7-hint-1, eval = FALSE}
linear_reg() |>
  fit(..., data = ...)
```

```{r transforming-variables-7-test, include = FALSE}
linear_reg() |>
  fit(income ~ age, data = trains)
```

### 

Since the model is linear, we assume that the relationship between `income` and `age` is stable. However, this is not true in reality, the effect when our age increases from 5 to 6 versus from 17 to 18, or 40 to 41 is not the same.

### Exercise 8

Behind the scenes, we have assigned the result of the pipe to an object named `fit_1`. Type `fit_1` and hit "Run Code." This will print a summary of the fitted model.

```{r transforming-variables-8, exercise = TRUE}

```

```{r transforming-variables-8-hint-1, eval = FALSE}
fit_1
```

```{r transforming-variables-8-test, include = FALSE}
fit_1
```

### 

In the Regression Coefficients section, we can see that the model returns the estimates and the 95% Confidence Intervals for the `Intercept` ($\beta_0$) and `age` ($\beta_1$). Recall that when calculating our quantity of interest, which is the coefficient in this case, we need to come up with a posterior probability distribution for our estimates.

### Exercise 9

Run `tidy()` on `fit_1` and hit "Run Code."

```{r transforming-variables-9, exercise = TRUE}

```

```{r transforming-variables-9-hint-1, eval = FALSE}
tidy(...)
```

```{r transforming-variables-9-test, include = FALSE}
tidy(fit_1)
```

### 

The value of $\beta_0$, the intercept in the regression, is `r scales::comma(round(tidy(fit_1) |> filter(term == "(Intercept)") |> pull(estimate), 0))`. This represents the estimated average income for a person with an age of zero, which is awkward and useless since there are no people of zero age in our data.

### Exercise 10

One way to make the intercept more meaningful is to transform `age`. Pipe `trains` to `mutate()` to create a new column `c_age = age - mean(age)`. Assign the result to `trains_2`.

```{r transforming-variables-10, exercise = TRUE}

```

```{r transforming-variables-10-hint-1, eval = FALSE}
... <- trains |> 
  mutate(... = age - ...(age))
```

```{r transforming-variables-10-test, include = FALSE}
trains_2 <- trains |> 
  mutate(c_age = age - mean(age))
```

### 

By doing this, we are subtracting the mean of a variable `age` in the data set from each of its values. The new distribution will be centered around zero meaning that it will have a mean of zero, but it will retain the same spread (standard deviation) and shape as the original distribution.

### Exercise 11

Copy your previous code, but set `formula` equal `income ~ c_age`, and `data` equal `trains_2`. Assign the result to an object called `fit_1_c`.

```{r transforming-variables-11, exercise = TRUE}

```

```{r transforming-variables-11-hint-1, eval = FALSE}
fit_1_c <- linear_reg() |>
  fit(... ~ ..., data = ...)
```

```{r transforming-variables-11-test, include = FALSE}
fit_1_c <- linear_reg() |>
  fit(income ~ c_age, data = trains_2)
```

### 

Using this centered version of age does not change the predictions or residuals in the model, but it does make the intercept easier to interpret. 

### Exercise 12

Type `fit_1_c` and hit "Run Code." This will print a summary of the fitted model. 

```{r transforming-variables-12, exercise = TRUE}

```

```{r transforming-variables-12-hint-1, eval = FALSE}
fit_1_c
```

```{r transforming-variables-12-test, include = FALSE}
fit_1_c
```

### 

Note that the estimate for the `Intercept` in this model is higher than that of the previous model that used the original values of `age` because the intercept now represents the expected value of the outcome variable `income` when `age` is at its mean value (instead of zero).

### Exercise 13

Run `tidy()` on `fit_1_c` and hit "Run Code."

```{r transforming-variables-13, exercise = TRUE}

```

```{r transforming-variables-13-hint-1, eval = FALSE}
tidy(fit_1_c)
```

```{r transforming-variables-13-test, include = FALSE}
tidy(fit_1_c)
```

### 

The intercept, `r scales::comma(round(tidy(fit_1_c) |> filter(term == "(Intercept)") |> pull(estimate), 0))`, is the expected income for someone with `c_age = 0`, i.e. someone of an average age in the data, which is around `r round(mean(trains$age), 0)`.

### 

Centering --- changing a variable via addition/subtraction --- often makes the intercept easier to interpret. Scaling --- changing a variable via multiplication/division --- often makes it easier to interpret coefficients. 

### Exercise 14

The most common scaling method is to divide the variable by its standard deviation. Start a pipe with `trains` to `mutate()` with the argument `s_age = age / sd(age)`. Assign the result to a new object called `trains_3`.

```{r transforming-variables-14, exercise = TRUE}

```

```{r transforming-variables-14-hint-1, eval = FALSE}
... <- trains |> 
  mutate(... = age / ...(age))
```

```{r transforming-variables-14-test, include = FALSE}
trains_3 <- trains |> 
  mutate(s_age = age / sd(age))
```

### 

By doing this, we are scaling the variable `age` so that its values are measured in terms of standard deviations from the mean. 

### Exercise 15

Create a model using the **tidymodels** framework. Use the formula `income ~ s_age` with the `trains_3` data. Assign the result to an object called `fit_1_s`.

```{r transforming-variables-15, exercise = TRUE}

```

```{r transforming-variables-15-hint-1, eval = FALSE}
... <- linear_reg() |>
  fit(... ~ ..., data = ...)
```

```{r transforming-variables-15-hint-2, eval = FALSE}
fit_1_s <- linear_reg() |>
  fit(income ~ s_age, data = trains_3)
```

### 

`s_age` is age scaled by its own standard deviation. A change in one unit of `s_age` is the same as a change in one standard deviation of the `age`, which is about 12.

### Exercise 16

Type `fit_1_s` and hit "Run Code." This will return a table of regression.

```{r transforming-variables-16, exercise = TRUE}

```

```{r transforming-variables-16-hint-1, eval = FALSE}
fit_1_s
```

```{r transforming-variables-16-test, include = FALSE}
fit_1_s
```

### 

In a standardized form, the coefficients of a regression model can be interpreted as the effect of a one standard deviation change in the predictor variable on the outcome variable. The interpretation of $\beta_1$ is now: 

> When comparing two people, one about 1 standard deviation worth of years older than the other, we expect the older person to earn about 11,000 more dollars.

### Exercise 17

Run `tidy()` on `fit_1_s` and hit "Run Code."

```{r transforming-variables-17, exercise = TRUE}

```

```{r transforming-variables-17-hint-1, eval = FALSE}
tidy(...)
```

```{r transforming-variables-17-test, include = FALSE}
tidy(fit_1_s)
```

### 

But, because we scaled without centering, the intercept is now back to the (nonsensical) meaning of the expected income for people of age 0.

### Exercise 18

The most common transformation applies both centering and scaling. Start a pipe with `trains` to `mutate` with the argument `z_age = scale(age)`. Assign the result to a new object called `trains_4`.

```{r transforming-variables-18, exercise = TRUE}

```

```{r transforming-variables-18-hint-1, eval = FALSE}
... <- ... |> 
  mutate(z_age = scale(...))
```

```{r transforming-variables-18-test, include = FALSE}
trains_4 <- trains |> 
  mutate(z_age = scale(age))
```

### 

The base R function `scale()` subtracts the mean and divides by the standard deviation. A variable so transformed is a “z-score”, meaning a variable with a mean of zero and a standard deviation of one.

### Exercise 19

Create a model using the **tidymodels** framework. Use the formula `income ~ z_age` with the `trains_4` data. Assign the result to an object called `fit_1_z`.

```{r transforming-variables-19, exercise = TRUE}

```

```{r transforming-variables-19-hint-1, eval = FALSE}
... <- linear_reg() |>
  fit(... ~ ..., data = ...)
```

```{r transforming-variables-19-hint-2, eval = FALSE}
fit_1_z <- linear_reg() |>
  fit(income ~ z_age, data = trains_4)
```

### 

Using *z*-scores makes interpretation easier, especially when we seek to compare the importance of different predictors. Note that, when using *z*-scores, we would often phrase this comparison in terms of “sigmas.” One person is “one sigma” older than another person means that they are one standard deviation older. 

### Exercise 20

Type `fit_1_z` and hit "Run Code." This will return a table of regression.

```{r transforming-variables-20, exercise = TRUE}

```

```{r transforming-variables-20-hint-1, eval = FALSE}
fit_1_z
```

```{r transforming-variables-20-test, include = FALSE}
fit_1_z
```

### 

The two parameters are easy to interpret after this transformation.

> The expected income of someone of average age, which is about `r round(mean(trains$age))` in this study, is about `r scales::comma(round(tidy(fit_1_z) |> filter(term == "Intercept") |> pull(estimate), -3))` dollars.

> When comparing two individuals who differ in age by one standard deviation, which is about `r round(sd(trains$age))` years in this study, the older person is expected to earn about `r scales::comma(round(tidy(fit_1_z) |> filter(term == "z_age") |> pull(estimate), -3))` dollars more than the younger.

### Exercise 21

Run `tidy()` on `fit_1_z` and hit "Run Code". 

```{r transforming-variables-21, exercise = TRUE}

```

```{r transforming-variables-21-hint-1, eval = FALSE}
tidy(...)
```

```{r transforming-variables-21-test, include = FALSE}
tidy(fit_1_z)
```

### 

Note that the term "sigma" might be confusing since we already using the word “sigma” to mean $\sigma$, the standard deviation of $\epsilon_i$. You will hear the same word "sigma" applied to both concepts, even in the same sentence. Determine meaning by context.

### Exercise 22

It is often helpful to take the log of predictor variables, especially in cases in which their distribution is skewed. We will be using the `kenya` data set from **primer.data**. Type `kenya` and hit "Run Code".

```{r transforming-variables-22, exercise = TRUE}

```

```{r transforming-variables-22-hint-1, eval = FALSE}
kenya
```

```{r transforming-variables-22-test, include = FALSE}
kenya
```

### 

Whether or not to take log of the variables may depend on the conventions in your field. If everyone does X, then you should probably do X, unless you have a good reason not to and need to explain it prominently.

### Exercise 23

Pipe `kenya` to `summary()` to return some statistics about the data.

```{r transforming-variables-23, exercise = TRUE}

```

```{r transforming-variables-23-hint-1, eval = FALSE}
... |> 
  summary()
```

```{r transforming-variables-23-test, include = FALSE}
kenya |> 
  summary()
```

### 

Let's say we are interested in the number of registered voters `rv13`. The summary shows that there are no missing values (NAs) in this column, however, there are rows with 0 voters, resulting in the min being 0.

### Exercise 24

To fix this, pipe `kenya` to the command `filter()` with the argument `rv13 > 0`. Assign the result to an object called `x`. 

```{r transforming-variables-24, exercise = TRUE}

```

```{r transforming-variables-24-hint-1, eval = FALSE}
... <- ... |> 
  filter(... > 0)
```

```{r transforming-variables-24-test, include = FALSE}
x <- kenya |> 
  filter(rv13 > 0)
```

### 

You should generally only take the log of variables for which all the values are strictly positive. The log of a negative number is not defined.

### Exercise 25

Next, we will look at the distribution of `rv13` in the our data. Behind the scene, we have created the graph for you. Type `rv_p` and hit "Run Code"

```{r transforming-variables-25, exercise = TRUE}

```

```{r transforming-variables-25-hint-1, eval = FALSE}
rv_p
```

```{r transforming-variables-25-test, include = FALSE}
rv_p
```

### 

Using the raw data, the distribution is heavily skewed to the right. However, we do not know the “true” model, who is to say that a model using the raw value is right or wrong?

### Exercise 26

Now let's see how the distribution looks like after transforming `rv13` into the log version. Type `log_rv_p` and hit "Run Code".

```{r transforming-variables-26, exercise = TRUE}

```

```{r transforming-variables-26-hint-1, eval = FALSE}
log_rv_p
```

```{r transforming-variables-26-test, include = FALSE}
log_rv_p
```

### 

The distribution after taking log is more symmetric and closer to a normal distribution. Check whether or not this choice meaningfully affects the answer to your question.

### Exercise 27

Let's put the two distributions next to each other to see the differences. Type `rv_p` and `log_rv_p`, connect them by `+`. Hit "Run Code"

```{r transforming-variables-27, exercise = TRUE}

```

```{r transforming-variables-27-hint-1, eval = FALSE}
... + ...
```

```{r transforming-variables-27-test, include = FALSE}
rv_p + log_rv_p
```

### 

Our inferences are often fairly "robust" to small changes in the model. If you get the same answer with `rv13` as from `log_rv13`, then no one cares which you use.

### Exercise 28

Lastly, add `title`, `subtitle` for the graph using the command `plot_annotation()`. 

```{r transforming-variables-28, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r transforming-variables-28-hint-1, eval = FALSE}
... + ... + 
  plot_annotation(title = ..., 
                  subtitle = ...)
```

Remember that this is what your graph should look like. 

```{r}
rv_p + log_rv_p +
  plot_annotation(title = 'Registered Votes In Kenya Communities',
                  subtitle = "Taking logs helps us deal with outliers")
```


```{r transforming-variables-28-test, include = FALSE}
rv_p + log_rv_p +
  plot_annotation(title = "Registered Votes In Kenya Communities",
                  subtitle = "Taking logs helps us deal with outliers")
```

### 

Most professionals, when presented with data distributed like `rv13`, would take the log. Professionals (irrationally?) hate outliers. Any transformation which makes a distribution look more normal is generally considered a good idea.

### Exercise 29

Instead of simply transforming variables, we can add more terms which are transformed versions of a variable. We will be using the `nhanes` data set. Type `nhanes` and hit "Run Code."

```{r transforming-variables-29, exercise = TRUE}

```

```{r transforming-variables-29-hint-1, eval = FALSE}
nhanes
```

```{r transforming-variables-29-test, include = FALSE}
nhanes
```

### 

The following exercises are built up in the way that the more complex our model is, the better predictions they make and thus the better answers to our questions they give. Complexity in this case can be referred to as the number of variables we put into our formulas, whether we transform them or use their raw versions, etc.

### Exercise 30

Consider the relation of `height` to `age` in `nhanes`. Pipe `nhanes` to `select()` with `height` and `age` as the arguments. 

```{r transforming-variables-30, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r transforming-variables-30-hint-1, eval = FALSE}
... |> 
  select(..., ...)
```

```{r transforming-variables-30-test, include = FALSE}
nhanes |> 
  select(height, age)
```

### 

Is `age` the best predictor of `height`? In the data set their are other potential variables such as `race` and `sex`, which one should we select, and which one should we discard? We will see in the later sections.

### Exercise 31

Let’s start by dropping the missing values. Copy your previous code, continue the pipe with `drop_na()`. Assign the result to an object called `no_na_nhanes`.

```{r transforming-variables-31, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r transforming-variables-31-hint-1, eval = FALSE}
... <- nhanes |> 
  select(..., ...) |> 
  ...
```

```{r transforming-variables-31-test, include = FALSE}
no_na_nhanes <- nhanes |> 
  select(height, age) |> 
  drop_na() 
```

### 

When building models, we rely on assumptions to construct the mathematical formula to illustrate the relationship between variables. The less assumptions we need to make about our model, the more robust it gets in terms of reflecting the true underlying patterns in the data.

### Exercise 32

Let's create a model to study the relationship between `height` and `age`. Create a linear regression model using the **tidymodels** framework. Use the formula `height ~ age` with the `no_na_nhanes` data. Assign the result to an object called `nhanes_1`.

```{r transforming-variables-32, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r transforming-variables-32-hint-1, eval = FALSE}
nhanes_1 <- linear_reg() |>
  fit(... ~ ..., data = ...)
```

```{r transforming-variables-32-test, include = FALSE}
nhanes_1 <- linear_reg() |>
  fit(height ~ age, data = no_na_nhanes)
```

### 

This model assumes the relationship between `age` and `height` is always constant, meaning that `height` always increases as `age` increases. That is not a very good model, obviously, we will see the reason why.

### Exercise 33

How accurate our model is can be determined by whether it represents the pattern of our data. Hit "Run Code". 

```{r transforming-variables-33, exercise = TRUE}
no_na_nhanes |>
  bind_cols(predict(nhanes_1, new_data = no_na_nhanes)) |>
  ggplot(aes(x = age, y = height)) +
  geom_point(alpha = 0.1) +
  geom_line(aes(y = .pred), color = "red", linewidth = 2) +
  labs(
    title = "Age and Height",
    subtitle = "Children are shorter, but a linear fit is poor",
    x = "Age",
    y = "Height (cm)",
    caption = "Data source: NHANES"
  )

```

```{r transforming-variables-33-test, include = FALSE}
no_na_nhanes |>
  bind_cols(predict(nhanes_1, new_data = no_na_nhanes)) |>
  ggplot(aes(x = age, y = height)) +
  geom_point(alpha = 0.1) +
  geom_line(aes(y = .pred), color = "red", linewidth = 2) +
  labs(
    title = "Age and Height",
    subtitle = "Children are shorter, but a linear fit is poor",
    x = "Age",
    y = "Height (cm)",
    caption = "Data source: NHANES"
  )
```

### 

From the graph, we see that before the age of 20, the data points gather and form a steep line, indicating that during this time period, people grow up really fast as their ages increase. However, after 20, there is little increase or even no increase in `height` as `age` increases. The red line created by the model does not capture this pattern. 

### Exercise 34
`
Run a linear regression using the **tidymodels** framework with a quadratic term. Use the formula `height ~ age + I(age^2)` and the `no_na_nhanes` dataset. Assign the result to an object called `nhanes_2`.

```{r transforming-variables-34, exercise = TRUE}

```

```{r transforming-variables-34-hint-1, eval = FALSE}
... <- linear_reg() |>
  fit(... ~ ... + I(...^2), data = ...)
```

```{r transforming-variables-34-test, include = FALSE}
nhanes_2 <- linear_reg() |>
  fit(height ~ age + I(age^2), data = no_na_nhanes)
```

### 

Note the need for `I()` in creating the squared term within the `formula` argument. Adding a quadratic term makes it better since we allow our model to get rid of the assumption of a linear relationship, which is not true between the two variables. 

### Exercise 35

Let's see whether this model is better in terms of demonstrating the relationship between `age` and `height` in our data set. Hit "Run Code".

```{r transforming-variables-35, exercise = TRUE}
no_na_nhanes |>
  bind_cols(predict(nhanes_2, new_data = no_na_nhanes)) |>
  arrange(age) |>    # sort after attaching predictions
  ggplot(aes(x = age, y = height)) +
    geom_point(alpha = 0.1) +
    geom_line(aes(y = .pred), color = "red") +
    labs(
      title = "Age and Height",
      subtitle = "Quadratic fit is much better, but still poor",
      x = "Age",
      y = "Height (cm)",
      caption = "Data source: NHANES"
    )
```

```{r transforming-variables-35-test, include = FALSE}
no_na_nhanes |>
  bind_cols(predict(nhanes_2, new_data = no_na_nhanes)) |>
  arrange(age) |>    # sort after attaching predictions
  ggplot(aes(x = age, y = height)) +
    geom_point(alpha = 0.1) +
    geom_line(aes(y = .pred), color = "red") +
    labs(
      title = "Age and Height",
      subtitle = "Quadratic fit is much better, but still poor",
      x = "Age",
      y = "Height (cm)",
      caption = "Data source: NHANES"
    )
```

### 

This line looks better, but not the best we could do. We have not used our background knowledge that people don't get any taller after age 18 or so. 

### Exercise 36

Let's create variables that capture the break at age 18. Create a linear regression model using the **tidymodels** framework. Use the formula `height ~ I(ifelse(age > 18, 18, age))` with the `no_na_nhanes` data. Assign the result to an object called `nhanes_3`.

```{r transforming-variables-36, exercise = TRUE}

```

```{r transforming-variables-36-hint-1, eval = FALSE}
... <- linear_reg() |>
  fit(height ~ ..., data = ...)
```

```{r transforming-variables-36-test, include = FALSE}
nhanes_3 <- linear_reg() |>
  fit(height ~ I(ifelse(age > 18, 18, age)), data = no_na_nhanes)
```

### 

Using the `ifelse()` command, we check every value of `age` in the data set, an original value is used if it is smaller than 18, every value greater than that threshold will be recorded as 18. The model does not understand the fact that people will (generally) not grow after the age of 18 so we need to manipulate this by "letting" it know that age-related changes in height are relevant only up to age 18.

### Exercise 37

Let's see how this model does in terms of demonstrating the relationship between the two variables. Hit "Run Code". 

```{r transforming-variables-37, exercise = TRUE}
no_na_nhanes |>
  bind_cols(predict(nhanes_3, new_data = no_na_nhanes)) |>
  arrange(age) |>
  ggplot(aes(x = age, y = height)) +
    geom_point(alpha = 0.1) +
    geom_line(aes(y = .pred), color = "red") +
    labs(
      title = "Age and Height",
      subtitle = "Domain knowledge makes for better models",
      x = "Age",
      y = "Height (cm)",
      caption = "Data source: NHANES"
    )
```

```{r transforming-variables-37-test, include = FALSE}
no_na_nhanes |>
  bind_cols(predict(nhanes_3, new_data = no_na_nhanes)) |>
  arrange(age) |>
  ggplot(aes(x = age, y = height)) +
    geom_point(alpha = 0.1) +
    geom_line(aes(y = .pred), color = "red") +
    labs(
      title = "Age and Height",
      subtitle = "Domain knowledge makes for better models",
      x = "Age",
      y = "Height (cm)",
      caption = "Data source: NHANES"
    )
```

### 

The new line correctly captures the pattern in our data, note that models only do what we tell them to do. We are the captains of our souls and using the knowledge we have to transform variables as needed.

## Selecting variables
### 

How do we decide which variables to include in a model? There is no one right answer to this question.

### Exercise 1

We will be using variables in the `trains` data set as an example. Type `trains` and hit "Run Code". The `trains` data set includes `gender`, `liberal`, `party`, `age`, `income`, `att_start`, `treatment`, `att_end`.

```{r selecting-variables-1, exercise = TRUE}

```

```{r selecting-variables-1-hint-1, eval = FALSE}
trains
```

```{r selecting-variables-1-test, include = FALSE}
trains
```

### 

Which variables would be best to include in a model depends on the question we are asking. For example, if we want to know if the ending attitude toward immigration differs between men and women, we need to include `gender` in the model.

### Exercise 2

Let's say we want to investigate the causal effect of exposing people to Spanish-speakers on their attitudes towards immigration. Pipe `trains` to `select()` with `att_end`, `treatment`, `att_start` as the arguments. 

```{r selecting-variables-2, exercise = TRUE}

```

```{r selecting-variables-2-hint-1, eval = FALSE}
... |> 
  select(..., ..., ...)
```

```{r selecting-variables-2-test, include = FALSE}
trains |> 
  select(att_end, treatment, att_start)
```

### 

We also keep X if the underlying theory/observation suggests that X has a meaningfully connection to the outcome variable. It can also be the case when it is standard in your field to include X in such regressions, or simply just because your boss wants to include X. 

### Exercise 3

Run a linear regression using the **tidymodels** framework. Use the formula `att_end ~ treatment + att_start` with the `trains` data. Assign the result to an object called `fit_1_model`.

```{r selecting-variables-3, exercise = TRUE}

```

```{r selecting-variables-3-hint-1, eval = FALSE}
... <- linear_reg() |>
  fit(..., data = ...)
```

```{r selecting-variables-3-hint-2, eval = FALSE}
fit_1_model <- linear_reg() |>
  fit(att_end ~ treatment + att_start, data = trains)
```
### 

`att_end`, the variable before the tilde, is our outcome. The explanatory variables are `treatment`, which says whether a commuter relieved treatment or control conditions, and `att_start`, which measures attitude at the start of the study.

### Exercise 4

Type `fit_1_model` and hit "Run Code." 

```{r selecting-variables-4, exercise = TRUE}

```

```{r selecting-variables-4-hint-1, eval = FALSE}
fit_1_model
```

```{r selecting-variables-4-test, include = FALSE}
fit_1_model
```

### 

Deciding if a variable is worth including in our model depends on whether it has a large and well-estimated coefficient. This means, roughly, that the 95% confidence interval excludes zero.

### Exercise 5

Run `tidy()` on `fit_1_model`. Hit "Run Code". 

```{r selecting-variables-5, exercise = TRUE}

```

```{r selecting-variables-5-hint-1, eval = FALSE}
tidy(...)
```

```{r selecting-variables-5-test, include = FALSE}
tidy(fit_1_model)
```

### 

The 95% confidence interval for the coefficient of `treatmentControl` is -1.43 to -0.45. This means that, if I compare two people, one of whom received the treatment (i.e., heard Spanish speakers on the platform) and one of whom did not, then we would expect the one who did not (i.e., the one who was a "control" in our experiment) to have a lower score after the experiment. 

### Exercise 6

How do we decide which variables are useful? First, let’s interpret our coefficients. In your own words, describe the magnitude of the relationship between the variable `treatmentControl` and our outcome variable, `att_end` and what that means. 

(Hint: Look at the sign of the estimate for that coefficient.)

```{r selecting-variables-6}
question_text(NULL,
	message = "The point estimate for `treatmentControl` is -0.95, indicating the negative effect of being in the control group (not being exposed to Spanish-speakers) on the attitude towards immigration at the end of the experiment.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The variable `treatmentControl` represents the *offset* in `att_end` from the estimate for our `Intercept`, which is for the group of people that were in the Control group. This means that, compared with the predicted `att_end` for groups under treatment, those in the control group have a predicted attitude that is almost one entire point lower.

### Exercise 7

In your own words, describe the magnitude of the relationship between `att_start` and `att_end`. Again, look at the sign of the estimated coefficient.

```{r selecting-variables-7}
question_text(NULL,
	message = "The estimated coefficient for `att_start` is positive, indicating that the attitude at the end of the experiment is higher for every one unit increase in `att_start`.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Recall that we keep variables that have a "large" coefficient, which can only be defined in the context of the specific model. Speaking roughly, removing a variable with a large coefficient meaningfully changes the predictions of the model.

### Exercise 8

Referring back to our regression table for `fit_1_model`, what is the 95% Confidence Interval for `treatmentControl`? Round it up to 2 decimal places.

```{r selecting-variables-8}
question_text(NULL,
	message = "The 95% CI for `treatmentControl` is [-1.44; -0.45].",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The 95% CI of this variables excludes zero, suggesting that it is a worthy variable to include in our model. 

### Exercise 9

What is the 95% Confidence Interval for `att_start`? Round it up to 2 decimal places.

```{r selecting-variables-9}
question_text(NULL,
	message = "The 95% CI for `att_start` is [0.72; 0.88].",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Applying the same requirement that the 95% Confidence Interval does NOT include zero, `att_start` is also a meaningful variable to our model. Therefore, our conclusion is to keep both variables, `treatment` and `att_start`. 

### Exercise 10

Let's us now consider `income` as a function of `age` and `liberal`, a proxy for political party. Pipe `trains` to `select()` with `income`, `age`, `liberal` as the arguments. 

```{r selecting-variables-10, exercise = TRUE}

```

```{r selecting-variables-10-hint-1, eval = FALSE}
... |> 
  select(..., ..., ...)
```

```{r selecting-variables-10-test, include = FALSE}
trains |> 
  select(income, age, liberal)
```

### 

The rule of thumb is to keep variables for which the estimated coefficient is more than two standard errors away from zero. Some of these variables won’t “matter” much to the model since their coefficients, although well-estimated, are small enough that removing the variable from the model will not affect the model’s predictions very much.

### Exercise 11

Run a linear regression using the **tidymodels** framework. Use the formula `income ~ age + liberal` with the `trains` data. Assign the result to an object called `fit_2_model`.

```{r selecting-variables-11, exercise = TRUE}

```

```{r selecting-variables-11-hint-1, eval = FALSE}
... <- linear_reg() |>
  fit(... ~ ..., data = ...)
```

```{r selecting-variables-11-hint-2, eval = FALSE}
fit_2_model <- linear_reg() |>
  fit(income ~ age + liberal, data = trains)
```

### 

The variable before the tilde, `income`, is our outcome. The explanatory variables are `liberal`, a logical value of TRUE or FALSE, and `age`, a numeric variable. 

### Exercise 12

Type `fit_2_model` and hit "Run Code." 

```{r selecting-variables-12, exercise = TRUE}

```

```{r selecting-variables-12-hint-1, eval = FALSE}
fit_2_model
```

```{r selecting-variables-12-test, include = FALSE}
fit_2_model
```

### 

The `Intercept` is estimating income where `liberal == FALSE`. Therefore, it is the estimated income for commuters that are not liberals and who have `age = 0`. The estimate for `age` is showing the increase in income with every additional year of age.

### Exercise 13

Run `tidy()` on `fit_2_model`.

```{r selecting-variables-13, exercise = TRUE}

```

```{r selecting-variables-13-hint-1, eval = FALSE}
tidy(...)
```

```{r selecting-variables-13-test, include = FALSE}

tidy(fit_2_model)
```

### 

The estimate for `liberalTRUE` represents the offset in predicted income for commuters who *are* liberal. To find the estimate, we must add the coefficient to our `Intercept` value. We see that, on average, liberal commuters make less money. 

### Exercise 14

In your own words, describe the magnitude of the relationship between `liberalTRUE` and `income`. Again, look at the sign of the estimated coefficient.

```{r selecting-variables-14}
question_text(NULL,
	message = "The coefficient for `liberalTRUE` is -32434 (negative), indicating that the liberals in our data set make less on average than non-liberals.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

It is important to note that we are not looking at a causal relationship for either `liberal` or `age`. We are noting the differences between two groups, without considering causality, recall the differences between *across unit* comparison and *within unit* comparison. 

### Exercise 15

What is the 95% Confidence Interval for `liberalTRUE`. Round it up to 2 decimal places.

```{r selecting-variables-15}
question_text(NULL,
	message = "The 95% CI for `liberalTRUE` is [-59756.45; -5266.50].",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

With rough mental math, we see that the 95% confidence interval excludes 0. Therefore, `liberal` is a helpful variable.

### Exercise 16

What is the 95% Confidence Interval for `age`. Round it up to 2 decimal places.

```{r selecting-variables-16}
question_text(NULL,
	message = "The 95% CI for `age` is [-22.75; 2177.09].",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The variable `age`, however, does not appear to have a meaningful impact on our `Intercept` since its coefficient is low and the 95% confidence interval *does not* exclude 0. Therefore, keep `liberal`, `age` is not that meaningful so it is really a matter of preference at this point. 

## Comparing models in theory
### 

Deciding which variables to include in a model is a subset of the larger question: How do we decide which model, out of the set of possible models, to choose?

### Exercise 1

Consider the first model, which seeks to explain the attitudes towards immigration among Boston commuters, using political ideology (`liberal`) as the explanatory variable. Run a linear regression using the **tidymodels**. Use the formula `att_end ~ liberal` with the `trains` data. Assign the result to an object called `fit_liberal`.

```{r comparing-models-in-theory-1, exercise = TRUE}

```

```{r comparing-models-in-theory-1-hint-1, eval = FALSE}
fit_liberal <- linear_reg() |>
  fit(... ~ ..., data = ...)
```

```{r comparing-models-in-theory-1-test, include = FALSE}
fit_liberal <- linear_reg() |>
  fit(att_end ~ liberal, data = trains)
```

### 

A better model not only makes better predictions on the current data set but also makes better prediction on the new data. For instance, if we were to predict how someone’s attitude changes toward immigration among Boston commuters based on political affiliation, we would want to go out and test our theories on new Boston commuters.

### Exercise 2

Run `tidy()` on `fit_liberal` and hit "Run Code".

```{r comparing-models-in-theory-2, exercise = TRUE}

```

```{r comparing-models-in-theory-2-hint-1, eval = FALSE}
tidy(...)
```

```{r comparing-models-in-theory-2-test, include = FALSE}
tidy(fit_liberal)
```

### 

The magnitude between `liberalTRUE` and `att_end` is negative, which makes sense. People who are liberal have more liberal attitudes about immigration, so we would expect their `att_end` scores to be lower. 

### Exercise 3

Consider another model which seeks to explain the attitudes towards immigration among Boston commuters, using `att_start` as the explanatory variable. Run a linear regression using **tidymodels** . Use the formula `att_end ~ att_start` with the `trains` data. Assign the result to an object called `fit_att_start`.

```{r comparing-models-in-theory-3, exercise = TRUE}

```

```{r comparing-models-in-theory-3-hint-1, eval = FALSE}
fit_att_start <- linear_reg() |>
  fit(... ~ ..., data = ...)
```

```{r comparing-models-in-theory-3-test, include = FALSE}
fit_att_start <- linear_reg() |>
  fit(att_end ~ att_start, data = trains)
```

###

When thinking of generalizing our model's prediction to new data, it is important to consider what is relevant new data in the context of the modeling problem. Some models are used to predict the future and, in those cases, we can wait and eventually observe the future and check how good our model is for making predictions.

### Exercise 4

<!-- DK: Replace `tidy()` and similar functions with tidy() equivalents. Done-->

Run `tidy()` on `fit_att_start` and hit "Run Code". 

```{r comparing-models-in-theory-4, exercise = TRUE}

```

```{r comparing-models-in-theory-4-hint-1, eval = FALSE}
tidy(...)
```

```{r comparing-models-in-theory-4-test, include = FALSE}
tidy(fit_att_start)
```

### 

We would also expect people to provide similar answers in two surveys administered a week or two apart. It makes sense that those with higher (more conservative) values for `att_start` would also have higher values for `att_end`.

### Exercise 5

The most obvious criterion for comparing models is the accuracy of their predictions. For example, consider using `liberal` to predict `att_end`. Pipe `trains` to `mutate()` and use the `predict()` function, which generates predicted values from your model for each observation. Assign these predicted values to a new column called `pred_liberal`.

```{r comparing-models-in-theory-5, exercise = TRUE}

```

```{r comparing-models-in-theory-5-hint-1, eval = FALSE}
trains |>
  mutate(... = predict(..., new_data = ...)$.pred)
```

```{r comparing-models-in-theory-5-test, include = FALSE}
trains |>
  mutate(pred_liberal = predict(fit_liberal, new_data = trains)$.pred)
```

### 

By doing this, you are creating a new column named `pred_liberal` that contains the predicted values for `att_end` from your `fit_liberal` model. The `.pred` column from `predict()` holds these model-generated predictions.

### Exercise 6

Copy the previous code, continue the pipe to `select()` and choose `pred_liberal` and `att_end` as the argument. 

```{r comparing-models-in-theory-6, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-6-hint-1, eval = FALSE}
... |> 
  select(att_end, pred_liberal)
```

```{r comparing-models-in-theory-6-test, include = FALSE}
trains |>
  mutate(pred_liberal = predict(fit_liberal, new_data = trains)$.pred) |>
  select(att_end, pred_liberal)
```

### 

The `att_end` column represents the actual attitude towards immigration at the end of the experiment, while the `pred_liberal` generates the predicted values for `att_end`. Note that there are rows when the prediction matches the actual values, but that's not always the case.

### Exercise 7

Copy your code from Exercise 5, continue the pipe with `ggplot()`. Map `x` to `pred_liberal`, `y` to `att_end` in the `aes()` argument. This will return a plain graph with two axes. 

```{r comparing-models-in-theory-7, exercise = TRUE}

```

```{r comparing-models-in-theory-7-hint-1, eval = FALSE}
... |>
  ggplot(aes(x = pred_liberal, y = att_end))
```

```{r comparing-models-in-theory-7-test, include = FALSE}
trains |>
  mutate(pred_liberal = predict(fit_liberal, new_data = trains)$.pred) |>
  ggplot(aes(x = pred_liberal, y = att_end))
```

### 

Each data point in the `x` axis is the predicted value for `att_end` of an individual and correspond to the actual value in the `y` axes. For some individuals, these are perfect predictions, but for others, they are poor predictions.

### Exercise 8

Copy the previous code, add `geom_jitter()` as a another layer. Set `width` equal to `0.05`, `height` equal to `0.2`, and `alpha` equal to `0.5`. 

```{r comparing-models-in-theory-8, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-8-hint-1, eval = FALSE}
... +
  geom_jitter(width = 0.05, height = 0.2, alpha = 0.5)
```

```{r comparing-models-in-theory-8-test, include = FALSE}
trains |> 
  mutate(pred_liberal = predict(fit_liberal, new_data = trains)$.pred) |> 
  ggplot(aes(x = pred_liberal, y = att_end)) +
    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5)
```

### 

Because there are only two possible values for `liberal` --- TRUE and FALSE --- there are only two predictions which this model will make: about 10 for `liberal == FALSE` and about 8 for `liberal == TRUE`. (The points in the above plot are jittered.)

### Exercise 9

Copy the previous code, add the `annotate()` command as another layer. The first argument is `"point"`, set `x` equal to `8`, `y` equal to `8`, `size` equal to `20`, `pch` equal to `1`, and `color` equal to `"red"`. 

```{r comparing-models-in-theory-9, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-9-hint-1, eval = FALSE}
... + 
  annotate("point", x = 8, y = 8, size = 20, pch = 1, color = "red")

```

```{r comparing-models-in-theory-9-test, include = FALSE}
trains |> 
  mutate(pred_liberal = predict(fit_liberal, new_data = trains)$.pred) |> 
  ggplot(aes(x = pred_liberal, y = att_end)) +
    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) + 
    annotate("point", x = 8, y = 8, size = 20, pch = 1, color = "red")
```

### 

By running this command, we add a red circle where our predictions are most accurate (where x and y values are the same, which is where our predictions equal the true attitudes).

### Exercise 10

Copy the previous code, add another `annotate()` command. The first argument is `"point"`, set `x` equal to `10`, `y` equal to `10`, `size` equal to `20`, `pch` equal to `1`, and `color` equal to `"red"`. Setting `pch` equal to `1` makes the inside of the point translucent to show the number of correct predictions. 

```{r comparing-models-in-theory-10, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-10-hint-1, eval = FALSE}
... +
  annotate("point", x = 10, y = 10, size = 20, pch = 1, color = "red")
```

```{r comparing-models-in-theory-10-test, include = FALSE}
trains |> 
  mutate(pred_liberal = predict(fit_liberal, new_data = trains)$.pred) |> 
  ggplot(aes(x = pred_liberal, y = att_end)) +
    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) + 
    annotate("point", x = 8, y = 8, size = 20, pch = 1, color = "red") +
    annotate("point", x = 10, y = 10, size = 20, pch = 1, color = "red")
```

### 

As we can see, the model isn't great at predicting `att_end`. (Note the two individuals who are `liberal == TRUE`, and who the model thinks will have `att_end == 8`, but who have `att_end == 15`. The model got them both very, very wrong.)

### Exercise 11

Finally, add `title`, `subtitle`, labels for `x` and `y`. 

```{r comparing-models-in-theory-11, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-11-hint-1, eval = FALSE}
... + 
  labs(title = ...,
       subtitle = ..., 
       x = ..., 
       y = ...)
```

Remember that your graph should look like this. 

```{r}
trains |> 
  mutate(pred_liberal = predict(fit_liberal, new_data = trains)$.pred) |> 
  ggplot(aes(x = pred_liberal, y = att_end)) +
    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) +
    annotate("point", x = 8, y = 8, size = 20, pch = 1, color = "red") +
    annotate("point", x = 10, y = 10, size = 20, pch = 1, color = "red") +
    labs(title = "Modeling Attitude Toward Immigration",
         subtitle = "Liberals are less conservative",
         x = "Predicted Attitude",
         y = "True Attitude")
```

```{r comparing-models-in-theory-11-test, include = FALSE}
trains |> 
  mutate(pred_liberal = predict(fit_liberal, new_data = trains)$.pred) |> 
  ggplot(aes(x = pred_liberal, y = att_end)) +
    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) +
    annotate("point", x = 8, y = 8, size = 20, pch = 1, color = "red") +
    annotate("point", x = 10, y = 10, size = 20, pch = 1, color = "red") +
    labs(title = "Modeling Attitude Toward Immigration",
         subtitle = "Liberals are less conservative",
         x = "Predicted Attitude",
         y = "True Attitude")
```

### 

The most sensible way to test a model is to use it to make predictions and compare those predictions to new data. After fitting the model using **tidymodels**, we use the `predict()` function to generate predicted values for new or existing cases.

### Exercise 12

Consider another model, using `att_start` to forecast `att_end`. Pipe `trains` to `mutate()` and use `predict(fit_att_start, new_data = trains)` to get the predicted values. Assign the predicted values to a new column called `pred_as`.

```{r comparing-models-in-theory-12, exercise = TRUE}

```

```{r comparing-models-in-theory-12-hint-1, eval = FALSE}
trains |> 
  mutate(pred_as = predict(..., new_data = trains)$`.pred`)
```

```{r comparing-models-in-theory-12-test, include = FALSE}
trains |> 
  mutate(pred_as = predict(fit_att_start, new_data = trains)$.pred)
```

### 

Similar to the previous model, we will create a new column that contains the predicted values for `att_end`, but from a different model `fitt_att_start`. Then we also compare our predictions with the actual values.

### Exercise 13

Copy the previous code, continue the pipe to `select()` and choose `pred_as` and `att_end` as the argument. 

```{r comparing-models-in-theory-13, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-13-hint-1, eval = FALSE}
... |> 
  select(pred_as, att_end)
```

```{r comparing-models-in-theory-13-test, include = FALSE}
trains |> 
  mutate(pred_as = predict(fit_att_start, new_data = trains)$.pred) |> 
  select(pred_as, att_end)
```

###

Note that the predictions can either be larger or smaller, or in some cases, equal to the actual values. If our predicted variable is smaller than the actual value, then we have underestimated the actual value, and vice versa. 

### Exercise 14

Copy your code from Exercise 12, continue the pipe with `ggplot()`. Map `x` to `pred_as`, `y` to `att_end` in the `aes()` argument. This will return a plain graph with two axes. 

```{r comparing-models-in-theory-14, exercise = TRUE}

```

```{r comparing-models-in-theory-14-hint-1, eval = FALSE}
... |> 
  ggplot(...(x = ..., y = ...))
```

```{r comparing-models-in-theory-14-test, include = FALSE}
trains |>
  mutate(pred_as = predict(fit_att_start, new_data = trains)$.pred) |>
  ggplot(aes(x = pred_as, y = att_end))
```

### 

Because `att_end` takes on `r length(unique(trains$att_end))` unique values, the model makes `r length(unique(trains$att_end))` unique predictions. Some of those predictions are perfect! But others are very wrong.

### Exercise 15

Copy the previous code, add `geom_jitter()` as another layer. Set `width` equal to `0.05`, `height` equal to `0.2`, and `alpha` equal to `0.5`. 

```{r comparing-models-in-theory-15, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-15-hint-1, eval = FALSE}
... +
  geom_jitter(width = 0.05, height = 0.2, alpha = 0.5)
```

```{r comparing-models-in-theory-15-test, include = FALSE}
trains |> 
  mutate(pred_as = predict(fit_att_start, new_data = trains)$.pred) |> 
  ggplot(aes(x = pred_as, y = att_end)) +
    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5)
```

###

Note that when we map the data points, they are spread through out the graph instead of only centering around specific values like the previous model. This is because `liberal` only takes 2 variable (0 and 1), while `att_start` can vary, which leads to more variation in the predicted values. 

### Exercise 16

Copy the previous code, add `geom_abline()` as another layer. Set `intercept` equal to `0`, `slope` equal to `1`, and `color` equal to `"red"`. This will insert a red line where our predictions are correct using `geom_abline()` with an intercept, slope and color.

```{r comparing-models-in-theory-16, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-16-hint-1, eval = FALSE}
... +
  geom_abline(intercept = 0, slope = 1, color = "red")
```

```{r comparing-models-in-theory-16-test, include = FALSE}
trains |> 
  mutate(pred_as = predict(fit_att_start, new_data = trains)$.pred) |> 
  ggplot(aes(x = pred_as, y = att_end)) +
    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) + 
    geom_abline(intercept = 0, slope = 1, color = "red")
```

<!-- AC: There's no hint. Done  -->

### 

Note the individual with a predicted `att_end` of around `9` but with an actual value of `15`. That is a big miss!

### Exercise 17

Finally, add `title`, `subtitle`, labels for `x` and `y` axes for your graph. 

```{r comparing-models-in-theory-17, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-17-hint-1, eval = FALSE}
... + 
  labs(title = ..., 
       subtitle = ...,
       x = ...,
       y = ...)
```

Remember this is what your graph should look like. 

```{r}
trains |> 
  mutate(pred_as = predict(fit_att_start, new_data = trains)$.pred) |> 
  ggplot(aes(x = pred_as, y = att_end)) +
    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) + 
    geom_abline(intercept = 0, slope = 1, color = "red") +
    labs(title = "Modeling Attitude Toward Immigration",
         subtitle = "Survey responses are somewhat consistent",
         x = "Predicted Attitude",
         y = "True Attitude")
```

```{r comparing-models-in-theory-17-test, include = FALSE}
trains |> 
  mutate(pred_as = predict(fit_att_start, new_data = trains)$.pred) |> 
  ggplot(aes(x = pred_as, y = att_end)) +
    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) + 
    geom_abline(intercept = 0, slope = 1, color = "red") +
    labs(title = "Modeling Attitude Toward Immigration",
         subtitle = "Survey responses are somewhat consistent",
         x = "Predicted Attitude",
         y = "True Attitude")
```

### 

Rather than looking at individual cases, we need to look at the errors for all the predictions. Fortunately, a prediction error is the same thing as a residual, which is easy enough to calculate.

### Exercise 18

Pipe `trains` to `select()` with `att_end`, `att_start`, and `liberal`. 

```{r comparing-models-in-theory-18, exercise = TRUE}

```

```{r comparing-models-in-theory-18-hint-1, eval = FALSE}
... |> 
  ...(..., ..., ...)
```

```{r comparing-models-in-theory-18-test, include = FALSE}
trains |> 
  select(att_end, att_start, liberal)
```

### 

A residual is the difference between the observed value of the dependent variable (actual value) and the predicted value (fitted value) produced by a regression model.

### Exercise 19

Continue the pipe with `mutate()`. Set `pred_lib` equal to the predicted values from `predict(fit_liberal, new_data = trains)`.

```{r comparing-models-in-theory-19, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-19-hint-1, eval = FALSE}
... |> 
  mutate(pred_lib = predict(..., new_data = trains)$`.pred`)
```

```{r comparing-models-in-theory-19-test, include = FALSE}
trains |> 
  select(att_end, att_start, liberal) |> 
  mutate(pred_lib = predict(fit_liberal, new_data = trains)$.pred)
```

### 

In mathematical terms, for each observation $i$: 

$$Residual = y_i - \hat{y_i} $$
$y_i$ is the actual value of the dependent variable, which is `att_end` and $\hat{y_i}$ is the predicted value of `att_end` generated from the model.

### Exercise 20

Add another column `resid_lib` equal to `pred_lib - att_end` to the current tibble using the `mutate()` command. 

```{r comparing-models-in-theory-20, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-20-hint-1, eval = FALSE}
...|> 
  mutate(resid_lib = pred_lib - att_end)
```

```{r comparing-models-in-theory-20-test, include = FALSE}
trains |> 
  select(att_end, att_start, liberal) |> 
  mutate(pred_lib = predict(fit_liberal, new_data = trains)$.pred) |> 
  mutate(resid_lib = pred_lib - att_end)
```

### 

Larger residuals mean larger deviations from the actual values. Smaller residuals indicate a model that fits the data well.

### Exercise 21

Continue the pipe with `mutate()`, create a new column called `pred_as` and set it equal to the predicted values from `fit_att_start` using the `predict()` function.

```{r comparing-models-in-theory-21, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-21-hint-1, eval = FALSE}
... |> 
  mutate(pred_as = predict(fit_att_start, new_data = trains)$`.pred`)
```

```{r comparing-models-in-theory-21-test, include = FALSE}
trains |> 
  select(att_end, att_start, liberal) |> 
  mutate(pred_lib = predict(fit_liberal, new_data = trains)$.pred) |> 
  mutate(resid_lib = pred_lib - att_end) |> 
  mutate(pred_as = predict(fit_att_start, new_data = trains)$.pred)
```

### 

Sadly, it is not wise to simply select the model which fits the data best because doing so can be misleading. We are using our data to select parameters and then, after using the data once, turning around and “checking” to see how well your model fits the data. It better fit! 

### Exercise 22

Finally, calculate the residual when using `att_start` to forecast `att_end`. Using `mutate()`, add `resid_as` and set it equal to `pred_as - att_end`.

```{r comparing-models-in-theory-22, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-22-hint-1, eval = FALSE}
... |> 
  mutate(resid_as = pred_as - att_end)
```

```{r comparing-models-in-theory-22-test, include = FALSE}
trains |> 
  select(att_end, att_start, liberal) |> 
  mutate(pred_lib = predict(fit_liberal, new_data = trains)$.pred) |> 
  mutate(resid_lib = pred_lib - att_end) |> 
  mutate(pred_as = predict(fit_att_start, new_data = trains)$.pred) |> 
  mutate(resid_as = pred_as - att_end)
```

<!-- AC: There's no hint. Done  -->

### 

*If the only criteria we cared about was how well the model predicts using the data on which the parameters were estimated, then a model with more parameters will always be better*. But that is not what truly matters. What matters is how well the model works on data which was not used to create the model.

### Exercise 23

Let's look at the square root of the average squared error. Pipe `trains` to `select` to take out `att_end`, `att_start` and `liberal`.

```{r comparing-models-in-theory-23, exercise = TRUE}

```

```{r comparing-models-in-theory-23-hint-1, eval = FALSE}
... |> 
  ...(..., ..., ...) 
```

```{r comparing-models-in-theory-23-test, include = FALSE}
trains |> 
  select(att_end, att_start, liberal) 
```

### 

There are many different measures of the error which we might calculate. The squared difference is most common for historical reasons: it was the mathematically most tractable in the pre-computer age. 

### Exercise 24

Continue the pipe with `mutate()`. Create a column named `lib_err` and set it equal to `(pred_lib - att_end)^2`. First, get the predictions using `predict(fit_liberal, new_data = trains)$.pred`.

```{r comparing-models-in-theory-24, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-24-hint-1, eval = FALSE}
... |>
  mutate(lib_err = (pred_lib - att_end)^2)
```

```{r comparing-models-in-theory-24-test, include = FALSE}
trains |> 
  select(att_end, att_start, liberal) |> 
  mutate(pred_lib = predict(fit_liberal, new_data = trains)$.pred) |>
  mutate(lib_err = (pred_lib - att_end)^2) 
```

###

Since the residuals can be negative, we could get a zero value when trying to sum them up, or we might be unable to take the square root of the difference if it's negative. Thus, we square them up so that all values are positive.

### Exercise 25

Continue the pipe with `mutate()`. Create a column named `as_err` and set it equal to `(pred_as - att_end)^2`. First, get the predictions using `predict(fit_att_start, new_data = trains)$.pred`.

```{r comparing-models-in-theory-25, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-25-hint-1, eval = FALSE}
... |> 
  mutate(as_err = (pred_as - att_end)^2)
```

```{r comparing-models-in-theory-25-test, include = FALSE}
trains |> 
  select(att_end, att_start, liberal) |> 
  mutate(pred_lib = predict(fit_liberal, new_data = trains)$.pred) |>
  mutate(lib_err = (pred_lib - att_end)^2) |>
  mutate(pred_as = predict(fit_att_start, new_data = trains)$.pred) |>
  mutate(as_err = (pred_as - att_end)^2)
```

###

Having calculated a squared difference for each observation, we can sum them or take their average or take the square root of their average. 

### Exercise 26

Continue the pipe with `summarize()`. The first argument is `lib_sigma` and set it equal to `sqrt(mean(lib_err))`, the second argument is `as_sigma` and set it equal to `sqrt(mean(as_err))`.

```{r comparing-models-in-theory-26, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r comparing-models-in-theory-26-hint-1, eval = FALSE}
... |> 
  summarize(lib_sigma = sqrt(mean(lib_err)),
            as_sigma = sqrt(mean(as_err)))
```

```{r comparing-models-in-theory-26-test, include = FALSE}
trains |> 
  select(att_end, att_start, liberal) |> 
  mutate(pred_lib = predict(fit_liberal, new_data = trains)$.pred) |>
  mutate(lib_err = (pred_lib - att_end)^2) |>
  mutate(pred_as = predict(fit_att_start, new_data = trains)$.pred) |>
  mutate(as_err = (pred_as - att_end)^2) |>
  summarize(lib_sigma = sqrt(mean(lib_err)),
            as_sigma = sqrt(mean(as_err)))

```

###

The better model is the one with a smaller square root of the residual. The `as_sigma` has a lower value compared to `lib_sigma` and thus it is a better model. 

## Summary
### 

This tutorial covers [Chapter 7: Mechanics](https://ppbds.github.io/primer/mechanics.html) of [*Preceptor’s Primer for Bayesian Data Science: Using the Cardinal Virtues for Inference*](https://ppbds.github.io/primer/) by [David Kane](https://davidkane.info/). 


 

```{r download-answers, child = system.file("child_documents/download_answers.Rmd", package = "tutorial.helpers")}
```
