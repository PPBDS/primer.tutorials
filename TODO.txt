Make more use of RStudio User Guide: https://docs.posit.co/ide/user/

## Some notes on branching.

* There is no simple way to do branching from the Console. Even with **usethis**, there isn't much.

* Not sure what the usethis::git_default_branch* family of functions do.

* I started a new branch from the Terminal with

Davids-MBP:all.primer.tutorials dkane$ git checkout -b data-overview
Switched to a new branch 'data-overview'
Davids-MBP:all.primer.tutorials dkane$

* Refreshing the Git pane switches us over. Pushing/pulling no longer work. Why?

* Once we are done with a branch, we can merge it back with:

git checkout master
git merge data-overview -m "some message"

I think.

## Utilities

In the tutorial.Rmd YAML header should:

progressive:  true
    allow_skip::  true

Replace the "yes" with TRUE. I bet it does not matter.


### Questions

I want to push the branch so that, if my computer blows up, I don't lose all this work. How?

## To-Do

Add a data-overview tutorial which matches the chapter more closely. This will probably pull some material from the current data tutorials, which is OK. We must have an overview tutorials for every chapter.

Remove group_by() from everything.

Ensure that the Census tutorials do an excellent job of making the same maps which we like to use in class during Week 3.

Warning: The `negative_parens` argument of `dollar()` is deprecated as of scales 1.2.0.
Please use the `style_negative` argument instead.

Check out PraireLearn. Looks really cool!

Reading Walkers book. Make three census tutorials.

Clean up Getting Help tutorial.

Clean Rubin Causal Model: Overview tutorial.

Clean up Wrangling - More Plots.

Overview tutorial for Chapter 3.

Use dashes not underscores in all filenames. Fix RStudio and Code/Github.

Add PDF tiny_tex() to which tutorial? RStudio and code?

Advanced: Get Format Code Chunk Labels add-in to catch the case in which a non-question code chunk is duplicated.

I think we could do more to reinforce the concepts of Preceptor Table and Population Table. Not sure of this should be done in a new Chapter 4 tutorial, or as more questions in the the current Overview tutorial. Or perhaps more reinforcement in the later tutorials. How? By having more examples, real world questions via which we guide students to give a series of short written answers. But, at each step, we give the correct answer so they don't fall too far behind. Start with a real world question. Figure out the units. Figure out the outcomes. Figure out the covariates. Phrase the problem in both a predictive and causal fashion. Decide which one you are using. Describe your Preceptor Table. Describe your Population Table. Walk through the Cardinal Virtues. In one sense, this is very repetitive. In another, it isn't! We want to force them to go through dozens of examples of real statistical issues and show how issues of validity, stability, representativeness, and unconfoundedness apply. Fluency in writing/speaking/thinking about these issues is a critical goal!

Update Tables tutorial. Does it include all the stuff from Making Tables section Tools chapter in Primer?

Overview tutorials for 3, 4, 5 and N parameters. Need lots of good examples of written questions and answers, making extensive use of the 4 cardinal virtues. Note the new title of the book.



## Later

Fix gh pages creation error.

Investigate this install message:

> remotes::install_github("PPBDS/primer.tutorials")
Downloading GitHub repo PPBDS/primer.tutorials@HEAD
Skipping 1 packages ahead of CRAN: stars


Testing for prep_rstudio_settings.

r-universe for making all.primer.tutorials easier to install.



Maybe r2u is as good or better than caching. https://eddelbuettel.github.io/r2u/

https://github.com/eddelbuettel/r2u/blob/master/inst/scripts/add_cranapt_jammy.sh



https://github.com/eddelbuettel/tidycpp/blob/master/.github/workflows/ci.yaml

https://community.rstudio.com/t/output-directory-in-quarto-cli-not-respected/143762/3

We should ensure that everything which can be tested is tested. For example, anytime we tell students to run X in the Console, we should silently run X as well, just to make sure it works. Failure to do so caused a bug when we told students to run `ggx:gghelp("Turn text 90 degrees")` which caused an error because there was only 1 colon in the command. There needs to be two colons. We especially need this testing in tutorials like RStudio and Code/Github because there are so many commands we require students to issue.



Fix the caching in R CMD check Github Action:

  https://github.com/r-lib/actions/tree/v2/examples#quickstart-ci-workflow

  https://docs.github.com/en/actions/using-workflows/caching-dependencies-to-speed-up-workflows#example-using-the-cache-action

  https://stackoverflow.com/questions/63521430/clear-cache-in-github-actions



## To discuss

https://stackoverflow.com/questions/63521430/clear-cache-in-github-actions

3) Automating tutorial workflow? Interested in connecting R with Google tools.


## Other stuff


Perhaps pull out Getting Started tutorial into its own package. Want this is be super easy to install.

* Note this message:

N  checking package dependencies (4.7s)
   Imports includes 40 non-default packages.
   Importing from so many packages makes the package vulnerable to any of
   them becoming unavailable.  Move as many as possible to Suggests and
   use conditionally.

Could we move all the packages which are used by the tutorials (except the core stuff, including primer.data) into Suggests? Students would then only install when needed. But then would the test cases pass in Github Actions?

* Note this message:

N  checking installed package size ...
     installed size is 87.8Mb
     sub-directories of 1Mb or more:
       doc         5.0Mb
       help        2.5Mb
       tutorials  80.2Mb

Should there really be 80 megs of data in the tutorials? Or is this the html pages?

### Visualization Links

These are handy links and ideas for the tutorials, mainly in week 1 and 2, which make plots. Many of the current plots are not that impressive.

http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html
https://cran.r-project.org/web/packages/ggside/vignettes/ggside_basic_usage.html

https://ggplot2.tidyverse.org/articles/

[A Gentle Guide to the Grammar of Graphics](https://pkg.garrickadenbuie.com/gentle-ggplot2) or https://moderndive.com/2-viz.html#grammarofgraphics

Tidy Tuesday: https://github.com/rfordatascience/tidytuesday

gghighlights, ggsides, ggrepel

Overview of the tutorial at the start. What functions are we covering? How do the sections related to each other? How many sections are there? How do they relate to each other? How does this tutorial relate to the other tutorials this week's?

Start providing the titles and subtitles in the question so that students could copy/paste them.

Put at the start and in the middle how long the tutorial is.


## Chapter 5 and Associated Tutorials

* Discuss tutorial which would simulate the NCAA basketball tournment. What are the odds of a 5th seed winning the whole thing? Follow the 538 approach of just simulating the whole tournament a lot. Or maybe something similar but simpler?

* Discuss simulating craps. Win with 7 or 11. Lose with 2, 3, or 12. Then roll until you make your "point" and win or "crap out" by rolling  a 7.

* Review new notation in chapter.

* Give the formula for the coin and President and height examples. The mathematical distribution is one, by definition, with a formula.


Notes on Permutation Tests

* Use "There is only one test" as one of the knowledge drops, perhaps in multiple parts. Include the graphic and link to the original. http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html

* Work through the entire simple example with N = 5. That makes things simple. Then, we do a second example, perhaps using something from primer.data, or from elsewhere. This is the approach we are currently following.





### Tutorials to add

Let's discuss the six broad categories of tutorials which we might add, especially to chapters 5 through 11.

First, we can just do more examples. Use different data with a different question, but all the same steps as in the chapter itself. Such examples will generally, like 082, do two analyses: one with all the data (to show the truth) and one in which representativeness fails in a way which we define, thereby showing how we can be led astray. Last step is to compare the answers from these two approaches, thus emphasizing the importance of humility.

Second, stand-alone tutorials on useful topics, like text modeling or networks or rtweet or whatever. These could go in any chapter and would not be restricted to having X number of parameters.

Third, we could cover many other model families, again perhaps not worrying too much about whether or not they have the "right" number of parameters. Simplest here would be other stan_glm families. That is, show a weird data set in which the outcome variable is something that should be fit with poisson or something, and then use family = poisson. Goal would be to emphasize that there are many possible models.

Fourth, expanding on the above, we might have some machine learning examples. After all, a machine-learning model is just a different kind of black box, not different conceptually than a linear or logistic model. We could fit a machine learning model on chapter 8 scenarios. Even though there are way more than three parameters in such models, they still can (I think!) be made to work in cases with just one input variable.

Fifth, real world data science examples which do not hide the messy complexity of the world. I guess that these would be a lot like the overview tutorials, but with 30 minutes of data cleaning first. These might also explicitly assume that you have done all the other tutorials in the chapter and that, therefore, we do not need to walk you through how to estimate a stan_glm() with five baby questions. Instead, the questions would be, not really harder, but messier.

Sixth, tutorials about interpreting parameters. We could have one of these in every chapter from 6 onward. These could be short! They would only cover parameter interpretation, perhaps only using the models which were used in the chapter. Or maybe a single big tutorial, for doing around chapter 10 or 11, which would cover all the messy details of interpreting parameters and also explaining why we should not spend much time on it.

Obviously, we could also combine these in various ways. Upon reflction, we should save the machine learning examples for later chapters in which we have several righhand side variables. Indeed, we could have three machine learning tutorials in chapter 11.

* Compare results from rstanarm model to an approach, like we learned in Chapter 5, in which we do the analysis "by hand," creating the joint distribution and then the posterior distribution. This is easiest to do for Chapter 6. Indeed, you can just take the code from Chapter 5, make it continuous by sampling lots of possible values, and then compare to rstanarm. Should work fine. More ambitiously, we could do this for Chapter 7. This requires a three dimensional joint distribution because there are two unknown parameters and then a two dimensional posterior, with one dimension for mu and one for sigma. This is cool! But perhaps over kill.

* Expand the Getting Started tutorial and make it better connected to the Primer chapter. Students read the chapter and then do the tutorial. The tutorial ought to confirm that they have done everything that the chapter told them to do. Even though it is done in class, it could be a bit longer, so 10 or 15 questions.


* A "models" which would be a full dry run of their final projects. Download some data (maybe lots of little data files) with an R script. Clean the data in other R script. Make the model (a simple two parameter model) in another R script. Then, make a quarto website which uses the model to make pretty pictures which answer a couple of questions. Should show results using gtsummary to make pretty tables of model results. Might even show how stan_glm() models differ from lm() models. Or maybe run stan_glm() with option to get the lm() results. Example usage: https://ipums.github.io/pma-data-hub/posts/2021-07-01-covid-tables/

* Advanced plotting, ideally focussed on all a variety of different plotting approaches designed for modeling-type questions and answers. Maybe https://feddelegrand7.github.io/ddplot/.

* Would it be possible to have tutorials in the last few weeks which were, more or less, final project check-ins . . .

* A tutorial about working with textual data. Perhaps based on tidytext book. Also: https://datavizs21.classes.andrewheiss.com/example/13-example/

* Regression to the mean

* The garden of forking paths

* M/S errors


* An rtweet tutorial: https://github.com/ropensci/rtweet

* A Python tutorial. See the new features in learnr: "Added a new polyglot tutorial to learnr. This tutorial displays mixing R, python, and sql exercises. See run_tutorial("polyglot", "learnr") for a an example. (#397)"


### To discuss

* Another example set: https://openintrostat.github.io/ims-tutorials/

* Specify which versions of learnr (and other Github packages?) we want. Otherwise, you always get the head, which may not be desirable. Do we really still need to be using the development version of learnr? That makes me nervous! And why haven't they updated the version on CRAN for more than 15 months?

* Best single summary of tutorial tools/approaches: https://damien-datasci-blog.netlify.app/post/how-to-make-an-interactive-tutorial-to-teach-r-an-overview/


* Another way to host the tutorials: https://higgi13425.github.io/medical_r/posts/2020-12-27-creating-mini-learnr-apps-and-hosting-on-a-server/


* mention iter = 10000

### To explore later

* Powerful approach for hosting cool looking tutorials: https://github.com/ines/course-starter-r. One example of a tutorial using this approach:  https://github.com/noamross/gams-in-r-course

* Should our tutorials look more like this one? https://minecr.shinyapps.io/dsbox-05-moneyinpolitics/. I don't think so. Too many words, not enough typing.

* Explore the use of setup chunks that are referenced by name, rather than requiring that the code chunk names match up. Example: exercise.setup = "setupA"

* Put the number of exercises in the group header so that students know how long? Or maybe put in in the exercise header in exercise 5, 10 and so on.

* Can we give students a search box in the tutorial so that they can find answers to questions they have already done?


* Interesting discussion and some plausible claims: http://laderast.github.io/2020/09/15/getting-learnr-tutorials-to-run-on-mybinder-org/. Claims that "the .Rmd file containing the tutorial should be named the same as the directory it is in." But why?

* https://github.com/karthik/holepunch is interesting. But it also hasn't been updated in more than 7 months.





### Allison Horst interview

The main question was: How do we incorporate tutorial questions directly into the chapter itself? Allison is the world expert on this topic.

* She is concerned about user loads for published Shiny apps - would recommend that we use the basic or higher plan.

* She thinks using Desiree's method of embedding one exercise at a time might get unreasonable for a book of our size, but suggested that we could host 12 different ShinyApps corresponding to each chapter, and then link to each chapter in one central location. (Kind of like the combined tutorials app Evelyn created via HMDC.)

* How did you decide which exercises were going to be "tutorials" and which would be "exercises"? She wanted the exercises to be incremental, so a lot of the code was already pre-populated. The exercises that were blank built directly off of code that was already shown before, so that students could have an "easy win" by just copying the previous code and tweaking a variable name. Allison thinks that this incremental process is important for beginner R students.

* How much memory does the ShinyApp consume? Do you know of any tricks to make learnr tutorials smaller? She recommends that we contact the team at RStudio Education - this is not her area of expertise. Allison says that the RStudio Education team is super eager to hear about learnr use cases, would be happy to talk to us, and would even ask us to write a blog post. She also mentioned that the team would know the most about cutting-edge learnr stuff, including things like a "completion rate" bar that shows students how far they are through a tutorial.

* Any tips for remote teaching or learning? Using learnr at all is a big step. Allison also pre-records short versions of her lectures for her students to stream at a later time,  and holds smaller discussion sections in class where they do activities.

* Allison's #1 Recommendation to remote teachers of R: Having students start out with RStudio Cloud and then transitioning to Desktop. You can set up workspaces so the necessary packages are already loaded and installed, any script is already pre-loaded, and all students need to do is log in. It looks exactly the same as RStudio Desktop except folks don't have to worry about installing R or RStudio. You can push/pull from git in RStudio Cloud as well (link to a Rstudio cloud tutorial/article: https://rstudio.com/resources/webinars/teaching-r-online-with-rstudio-cloud/)

## Videos Worth Looking At?

Data Science Mini Projects: https://www.youtube.com/playlist?list=PLpdmBGJ6ELUJM6_vKQbpi9vGc65Sn0uPr
Draw Multiple Time Series in Same Plot: https://www.youtube.com/watch?v=FNXnJ_zT1gE
Barplot in R: https://www.youtube.com/watch?v=pYbuWU77QkU&feature=youtu.be

# Interestin approach for next summer

## Material from JSM 2021

Education section: https://matackett.github.io/open-stat-ds-ed/



Interesting discussion of using art to bring students into data science.

Consider the use of ggirl package for encouraging students to send someone an actual postcard.

Mine's discussion of how to make html and pdf versions of knitted R markdown look the same. Use fenced div blocks and then special CSS and LaTeX code chunks. Details: https://speakerdeck.com/minecr/openintro-building-sustaining-supporting-and-growing-open-source-educational-resources?slide=13

## Discussion of Tutorials

Let's discuss the six broad categories of tutorials which we might add, especially to chapters 5 through 11.

First, we can just do more examples. Use different data with a different question, but all the same steps as in the chapter itself. Such examples will generally, like 082, do two analyses: one with all the data (to show the truth) and one in which representativeness fails in a way which we define, thereby showing how we can be led astray. Last step is to compare the answers from these two approaches, thus emphasizing the importance of humility.

Second, stand-alone tutorials on useful topics, like text modeling or networks or rtweet or whatever. These could go in any chapter and would not be restricted to having X number of parameters.

Third, we could cover many other model families, again perhaps not worrying too much about whether or not they have the "right" number of parameters. Simplest here would be other stan_glm families. That is, show a weird data set in which the outcome variable is something that should be fit with poisson or something, and then use family = poisson. Goal would be to emphasize that there are many possible models.

Fourth, expanding on the above, we might have some machine learning examples. After all, a machine-learning model is just a different kind of black box, not different conceptually than a linear or logistic model. We could fit a machine learning model on chapter 8 scenarios. Even though there are way more than three parameters in such models, they still can (I think!) be made to work in cases with just one input variable.

Fifth, real world data science examples which do not hide the messy complexity of the world. I guess that these would be a lot like the overview tutorials, but with 30 minutes of data cleaning first. These might also explicitly assume that you have done all the other tutorials in the chapter and that, therefore, we do not need to walk you through how to estimate a `stan_glm()` with five baby questions. Instead, the questions would be, not really harder, but messier.

Sixth, tutorials about interpreting parameters. We could have one of these in every chapter from 6 onward. These could be short! They would only cover parameter interpretation, perhaps only using the models which were used in the chapter.

Obviously, we could also combine these in various ways. Upon reflection, we should save the machine learning examples for later chapters in which we have several righthand side variables. Indeed, we could have three machine learning tutorials in chapter 11.

### Later tutorials in later chapters

The first tutorial for each of the later chapters is easy. Just do more or less exactly what the chapter does. Recall that, one day, we expect to incorporate those questions directly into the chapter itself. So, the more that it just forces students to type in the same commands as in the chapter, the better. But, for the most part, the chapters are not that long! There is only about one tutorial worth of material.

What do we do for the other 5 or so tutorials for each chapter? Good question! Honestly, I am not sure. Here are some ideas:

* The second tutorial could be very similar to the first -- and, therefore, to the chapter. Just use a different variable(s) and/or a different tibble, but answer the same sorts of questions in the same way.

* Don't be repetitive in the written questions. It is fine, in the first tutorial for each chapter, to ask students to write a definition of the Preceptor Table. Indeed, we should always have that question in each first tutorial. But, we don't want that question in each of the 5 tutorials for a single chapter.

* Use written questions which do require different answers depending in the data/variable used. We can't resuse "Define a Preceptor Table" because that question has the same answer every time. Consider a different question:

> Write a paragraph which explores reasons why we should **not** consider the Preceptor Table and our data set as being drawn from the same population?

That is a good question, not least because it needs a different answer for each data science problem we confront. Another good example:

> Write a paragraph explaining why, even though the data is not a random sample, we may still consider it to be "respsentative" enough to move forward?

Again, this requires a different answer for each new tibble.

* We don't usually (ever?) ask questions about the four Cardinal Virtues directly. They are simply a pedagogical device we use to help students organize their work. But we do ask written questions about the key concepts (population, representativeness, validity) in each and every tutorial. These are difficult concepts with no "right" answers. We need to wrestle with them every week.

* A tutorial after the first might do a very similar exercise but with three differences. First, it can go faster, given that students have already seen the basics fairly slowly in the first tutorial. Second, it can add some complexity, make the modeling problem just a little more difficult than the basic case. Third, it can add some new magic. Perhaps it grabs data using a new package instead of just using another boring data set from **primer.data**. Perhaps it creates a nicer plot with some geoms we have not used before.

* I will distribute the problem sets we used last semester. Turning them into tutorials might work well. But always remember that tutorials are easy, while problem sets are hard. So, we need to split up a big problem set question into 15 exercises and then walk the students slowly (and with hints) through those 10 exercises.

* Maybe there are easy ways, in tutorial 2 or 3, to take the problem we solved in tutorial 1 and have it loosen the assumption that the data we have is representative of the population. This is a nice assumption to loosen since it is never true. I am not sure how one would do this . . .

* If the main example is the chapter is one which uses a linear model, then tutorial 2 could use a logistic model and then tutorial 3 could use the third kind of model (which we have not chosen yet). Similarly, if the chapter (and first tutorial) does a predictive mode, then tutorial 3 should do a causal model.

* The last tutorial might, in some sense, try to set the stage for the next chapter, provide a teaser for what we are learning next.

* Another type of tutorial is one which uses fake data which has been manipulated to violate the assumptions which students are making. Show them Preceptor's Posterior and compare it to their own.
