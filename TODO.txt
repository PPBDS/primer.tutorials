### Madhavan

* Complete Tutorial 111. 

* Read: https://marginaleffects.com/chapters/categorical.html

Central issue: Why can't we identify which dots correspond to which responses when we have an ordinal model in marginaleffects?

### Khang

Complete the first nine chapters of tutorials in https://github.com/PPBDS/tidymodels.tutorials. Email me the answers.

Optional: Read first nine chapters https://www.tmwr.org/. Go fast if you have seen this
material before. 

Choose a dataset which might be fun to use.


Make a new tutorial 151. New directory. Use the template. 


Start by making model. What kind? Flexible.
  

### Satvika

make tutorial 141. New directory. Copy over template. 

Choose a dataset which might be fun to use. See list below.

Try fitting a random forest model.

Try using marginaleffects functions on the fitted model.

### Other Issues

Tutorial 9, which is a total mess.

Tutorial 10, less a mess.

### Possible Data Sets to Use for New Tutorials

These are all chosen to have thousands of observations and scores of variables, thereby 
allowing for machine learning models.

Source: https://vincentarelbundock.github.io/Rdatasets/articles/data.html

https://vincentarelbundock.github.io/Rdatasets/doc/openintro/labor_market_discrimination.html

https://vincentarelbundock.github.io/Rdatasets/doc/openintro/hfi.html

https://vincentarelbundock.github.io/Rdatasets/doc/ISLR/Caravan.html

https://vincentarelbundock.github.io/Rdatasets/doc/modeldata/ames.html

https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/Car.html

https://vincentarelbundock.github.io/Rdatasets/doc/AER/STAR.html

https://vincentarelbundock.github.io/Rdatasets/doc/sampleSelection/RandHIE.html

https://vincentarelbundock.github.io/Rdatasets/doc/modeldata/deliveries.html

https://vincentarelbundock.github.io/Rdatasets/doc/modeldata/hotel_rates.html

### Other Data Sets

Stereotype threat replication: https://osf.io/preprints/psyarxiv/qctkp

### Projects

#### extract_eq() problem in Tutorial 111

Really annoying that extract_eq() does not work inside of tutorial 111, meaning that it does not work in exercise code chunks for students and it does not work when we use it in a code chunk in the Rmd itself. Seems to be a problem with MASS:polr objects. Tried to create a simple replication and was not able to. But you can see the problem with the fit_approval as in tutorial 111. My guess is that this has something to do with gtsummary. At least, I don't think we had this problem before using gtsummary. Or maybe it is something weird with broom/broom.helpers, which both have a weird relationship with gtsummary. Note how we have to put broom.helpers in the Imports in the DESCRIPTION file, for no good reason. Note, also, that everything works fine in a qmd file, or even a simple Rmd file, but not in a "runtime: shiny_prerendered" file, I think. I am also suspicious of the two different versions of tidy() which float around, one in the broom package and one in the generics (??) package.

If/when we want to fix this, then we need to start by making the simplest possible reprex. My guess is that this needs to be a "runtime: shiny_prerendered" file which creates a MASS::polr object and then tries to run extract_eq() on it.


#### similar problems with plot_predictions

plot_predictions(,,,, draw = FALSE) working fine interactively with a MASS:polr object but failing to refit the Hession when run in a tutorial.




#### Other Items

ordinal package is being maintained, which is nice.

More new tutorials.

Would modelsummary be better than making tables with gt? Maybe that would make understanding dummy variables easier, because the table would automatically show the dummy variables as levels?

Need to add tidy() back to 4, 6, 8, 9 and 11.

extract_eq(lr, wrap = TRUE, show_distribution = TRUE)

Get MASS::polr() objects to work with equatiomatic.

Need to figure out how to explain that extract_eq on logistic_reg objects gives you the log-odds formulation.

Should we put the final image at the start of each tutorial, in the Introduction? Might it be interesting or motivating for students? Or a waste of time?

Change tutorial 11 to use different base model. Update it to new template.



### Projects to Consider




* Figure out pp_check() replacement.

* A new tutorial in primer.tutorials which just shows students how to find cool variables from the Census. The causal effect of the tutorial should be to make students more likely to use Census data for their projects.

* Consider switching categorical to multinomial and Bernoulli to binomial.

* Update 4 parameters so it does not produce these warning messages by default: https://rpubs.com/lelasengupta/four-parameters

* Explore gradetools: https://federicazoe.github.io/gradetools/index.html

* Deal with katex. Seems like all (?) Windows users are getting an error about needing the katex package. But nothing for Mac users. So, I added katex as an import, which is an absurd hack. Better approach?



* Make the Primer better and more connected to the tutorials. (This might be something that everyone ends up working on.) 

* Positron is the IDE of the future. Create Getting Started and RStudio Code/Github tutorials which use it. 

* I hope to have hundreds of students next summer. I need to work on tools now for managing that. See PraireLearn, Moodle, Google sheets and so on. 

  + 1) Where should students upload files? We currently use Google forms which puts stuff in a Google drive folder. This works pretty well! But I need to download the resulting folder by hand, which is no good. Surely, we can automate this. But can we automate it for all TFs. (We want TFs to be able to see and analyze the homeworks submitted by their students.) A second option would be DropBox. Others?
  
  + 2) Where do we keep track of the students in the class and key information associated with them? That information would be name, email, TF, breakout room, presentation room. Maybe only me and the head TFs need to edit it. I think that this might just be a Google sheet. We would want all staff to be able to access it since doing so would provide the info to allow scripts to just return a given TF's students' work.
  
  + 3) Where do we populate information about work completed? That is, we want to be able to report who has submitted each homework, perhaps also with the number of answers. We need to put that information somewhere, update it with scripts, and then allow TFs to be able to access it. 
  
  + 4) As a one-off, it would be nice to allow staff to save: Give me all the answers to this question for my students. Or for me to say the same about all students. Most common is to gather all the Rpub locations for student work. Even cooler would be to automatically open up a tab for each, with the tab given the name of the student.

* tutorial.helpers works well. But it could do a lot more! This might be a good project for someone with some coding experience. See https://github.com/PPBDS/tutorial.helpers/blob/main/TODO.txt


* Animation. Use the DGM to create one complete Preceptor Table. In that draw, Joe is a 6 for `att_end`. Then, do another draw. Joe is a 5. Do a thousand draws. You then have a thousand Preceptor Tables. Calculate the Quantity of Interest for each Preceptor Table. The 1,000 values are the posterior for your QoI. Would be great to make a cool animation of this, perhaps with a simple example. Would be fun to have a similar animation for each chapter. Great summer project!


## Other Stuff


Include somewhere:

There will often be several tutorials associated with a given chapter. The first is an "Overview" tutorial which follows closely along with the words and examples in the chapter itself. Someday, we will [put those exercises directly](https://github.com/tinystats/teacups-giraffes-and-statistics) into the *Primer* itself, so keeping the two in sync makes sense. We can group other tutorials into three broad categories. First, some tutorials go into more detail about certain aspects of the chapter. Second, some tutorials provide further examples/extensions of the main theme of the chapter as a whole. Third, the remaining tutorials cover a topic --- like Quarto Websites or Tables --- which has no necessary connection to the chapter itself.

## Some notes on branching.

* There is no simple way to do branching from the Console. Even with **usethis**, there isn't much.

* Not sure what the usethis::git_default_branch* family of functions do.

* I started a new branch from the Terminal with

Davids-MBP:primer.tutorials dkane$ git checkout -b data-overview
Switched to a new branch 'data-overview'
Davids-MBP:primer.tutorials dkane$

* Refreshing the Git pane switches us over. Pushing/pulling no longer work. Why?

* Once we are done with a branch, we can merge it back with:

git checkout master
git merge data-overview -m "some message"

I think.

### Questions

I want to push the branch so that, if my computer blows up, I don't lose all this work. How?

## To-Do


Check out PraireLearn. Looks really cool!


Add PDF tiny_tex() to which tutorial? RStudio and code? Or maybe typist makes this unnecessary?

Advanced: Get Format Code Chunk Labels add-in to catch the case in which a non-question code chunk is duplicated.


## Later

Testing for prep_rstudio_settings.

r-universe for making primer.tutorials easier to install.

Maybe r2u is as good or better than caching. https://eddelbuettel.github.io/r2u/

https://github.com/eddelbuettel/r2u/blob/master/inst/scripts/add_cranapt_jammy.sh

https://github.com/eddelbuettel/tidycpp/blob/master/.github/workflows/ci.yaml

https://community.rstudio.com/t/output-directory-in-quarto-cli-not-respected/143762/3

We should ensure that everything which can be tested is tested. For example, anytime we tell students to run X in the Console, we should silently run X as well, just to make sure it works. Failure to do so caused a bug when we told students to run `ggx:gghelp("Turn text 90 degrees")` which caused an error because there was only 1 colon in the command. There needs to be two colons. We especially need this testing in tutorials like RStudio and Code/Github because there are so many commands we require students to issue.


* Automating tutorial workflow? Interested in connecting R with Google tools.


## Other stuff


### Visualization Links

These are handy links and ideas for the tutorials, mainly in week 1 and 2, which make plots. Many of the current plots are not that impressive.

http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html
https://cran.r-project.org/web/packages/ggside/vignettes/ggside_basic_usage.html

https://ggplot2.tidyverse.org/articles/

[A Gentle Guide to the Grammar of Graphics](https://pkg.garrickadenbuie.com/gentle-ggplot2) or https://moderndive.com/2-viz.html#grammarofgraphics

Tidy Tuesday: https://github.com/rfordatascience/tidytuesday

gghighlights, ggsides, ggrepel

Overview of the tutorial at the start. What functions are we covering? How do the sections related to each other? How many sections are there? How do they relate to each other? How does this tutorial relate to the other tutorials this week's?

Start providing the titles and subtitles in the question so that students could copy/paste them.

Put at the start and in the middle how long the tutorial is.


## Chapter 5 and Associated Tutorials

* Discuss tutorial which would simulate the NCAA basketball tournment. What are the odds of a 5th seed winning the whole thing? Follow the 538 approach of just simulating the whole tournament a lot. Or maybe something similar but simpler?

* Discuss simulating craps. Win with 7 or 11. Lose with 2, 3, or 12. Then roll until you make your "point" and win or "crap out" by rolling  a 7.

* Review new notation in chapter.

* Give the formula for the coin and President and height examples. The mathematical distribution is one, by definition, with a formula.


Notes on Permutation Tests

* Use "There is only one test" as one of the knowledge drops, perhaps in multiple parts. Include the graphic and link to the original. http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html

* Work through the entire simple example with N = 5. That makes things simple. Then, we do a second example, perhaps using something from primer.data, or from elsewhere. This is the approach we are currently following.





### Tutorials to add

Let's discuss the six broad categories of tutorials which we might add, especially to chapters 5 through 11.

First, we can just do more examples. Use different data with a different question, but all the same steps as in the chapter itself. Such examples will generally, like 082, do two analyses: one with all the data (to show the truth) and one in which representativeness fails in a way which we define, thereby showing how we can be led astray. Last step is to compare the answers from these two approaches, thus emphasizing the importance of humility.

Second, stand-alone tutorials on useful topics, like text modeling or networks or rtweet or whatever. These could go in any chapter and would not be restricted to having X number of parameters.

Third, we could cover many other model families, again perhaps not worrying too much about whether or not they have the "right" number of parameters. Simplest here would be other stan_glm families. That is, show a weird data set in which the outcome variable is something that should be fit with poisson or something, and then use family = poisson. Goal would be to emphasize that there are many possible models.

Fourth, expanding on the above, we might have some machine learning examples. After all, a machine-learning model is just a different kind of black box, not different conceptually than a linear or logistic model. We could fit a machine learning model on chapter 8 scenarios. Even though there are way more than three parameters in such models, they still can (I think!) be made to work in cases with just one input variable.

Fifth, real world data science examples which do not hide the messy complexity of the world. I guess that these would be a lot like the overview tutorials, but with 30 minutes of data cleaning first. These might also explicitly assume that you have done all the other tutorials in the chapter and that, therefore, we do not need to walk you through how to estimate a stan_glm() with five baby questions. Instead, the questions would be, not really harder, but messier.

Sixth, tutorials about interpreting parameters. We could have one of these in every chapter from 6 onward. These could be short! They would only cover parameter interpretation, perhaps only using the models which were used in the chapter. Or maybe a single big tutorial, for doing around chapter 10 or 11, which would cover all the messy details of interpreting parameters and also explaining why we should not spend much time on it.

Obviously, we could also combine these in various ways. Upon reflction, we should save the machine learning examples for later chapters in which we have several righhand side variables. Indeed, we could have three machine learning tutorials in chapter 11.

* Compare results from rstanarm model to an approach, like we learned in Chapter 5, in which we do the analysis "by hand," creating the joint distribution and then the posterior distribution. This is easiest to do for Chapter 6. Indeed, you can just take the code from Chapter 5, make it continuous by sampling lots of possible values, and then compare to rstanarm. Should work fine. More ambitiously, we could do this for Chapter 7. This requires a three dimensional joint distribution because there are two unknown parameters and then a two dimensional posterior, with one dimension for mu and one for sigma. This is cool! But perhaps over kill.

* Expand the Getting Started tutorial and make it better connected to the Primer chapter. Students read the chapter and then do the tutorial. The tutorial ought to confirm that they have done everything that the chapter told them to do. Even though it is done in class, it could be a bit longer, so 10 or 15 questions.


* A "models" which would be a full dry run of their final projects. Download some data (maybe lots of little data files) with an R script. Clean the data in other R script. Make the model (a simple two parameter model) in another R script. Then, make a quarto website which uses the model to make pretty pictures which answer a couple of questions. Should show results using gtsummary to make pretty tables of model results. Might even show how stan_glm() models differ from lm() models. Or maybe run stan_glm() with option to get the lm() results. Example usage: https://ipums.github.io/pma-data-hub/posts/2021-07-01-covid-tables/

* Advanced plotting, ideally focussed on all a variety of different plotting approaches designed for modeling-type questions and answers. Maybe https://feddelegrand7.github.io/ddplot/.

* Would it be possible to have tutorials in the last few weeks which were, more or less, final project check-ins . . .

* A tutorial about working with textual data. Perhaps based on tidytext book. Also: https://datavizs21.classes.andrewheiss.com/example/13-example/

* Regression to the mean

* The garden of forking paths

* M/S errors


* An rtweet tutorial: https://github.com/ropensci/rtweet

* A Python tutorial. See the new features in learnr: "Added a new polyglot tutorial to learnr. This tutorial displays mixing R, python, and sql exercises. See run_tutorial("polyglot", "learnr") for a an example. (#397)"


### To discuss

* Another example set: https://openintrostat.github.io/ims-tutorials/

* Specify which versions of learnr (and other Github packages?) we want. Otherwise, you always get the head, which may not be desirable. Do we really still need to be using the development version of learnr? That makes me nervous! And why haven't they updated the version on CRAN for more than 15 months?

* Best single summary of tutorial tools/approaches: https://damien-datasci-blog.netlify.app/post/how-to-make-an-interactive-tutorial-to-teach-r-an-overview/


* Another way to host the tutorials: https://higgi13425.github.io/medical_r/posts/2020-12-27-creating-mini-learnr-apps-and-hosting-on-a-server/


* mention iter = 10000

### To explore later

* Powerful approach for hosting cool looking tutorials: https://github.com/ines/course-starter-r. One example of a tutorial using this approach:  https://github.com/noamross/gams-in-r-course

* Should our tutorials look more like this one? https://minecr.shinyapps.io/dsbox-05-moneyinpolitics/. I don't think so. Too many words, not enough typing.

* Explore the use of setup chunks that are referenced by name, rather than requiring that the code chunk names match up. Example: exercise.setup = "setupA"

* Put the number of exercises in the group header so that students know how long? Or maybe put in in the exercise header in exercise 5, 10 and so on.

* Can we give students a search box in the tutorial so that they can find answers to questions they have already done?


* Interesting discussion and some plausible claims: http://laderast.github.io/2020/09/15/getting-learnr-tutorials-to-run-on-mybinder-org/. Claims that "the .Rmd file containing the tutorial should be named the same as the directory it is in." But why?

* https://github.com/karthik/holepunch is interesting. But it also hasn't been updated in more than 7 months.





### Allison Horst interview

The main question was: How do we incorporate tutorial questions directly into the chapter itself? Allison is the world expert on this topic.

* She is concerned about user loads for published Shiny apps - would recommend that we use the basic or higher plan.

* She thinks using Desiree's method of embedding one exercise at a time might get unreasonable for a book of our size, but suggested that we could host 12 different ShinyApps corresponding to each chapter, and then link to each chapter in one central location. (Kind of like the combined tutorials app Evelyn created via HMDC.)

* How did you decide which exercises were going to be "tutorials" and which would be "exercises"? She wanted the exercises to be incremental, so a lot of the code was already pre-populated. The exercises that were blank built directly off of code that was already shown before, so that students could have an "easy win" by just copying the previous code and tweaking a variable name. Allison thinks that this incremental process is important for beginner R students.

* How much memory does the ShinyApp consume? Do you know of any tricks to make learnr tutorials smaller? She recommends that we contact the team at RStudio Education - this is not her area of expertise. Allison says that the RStudio Education team is super eager to hear about learnr use cases, would be happy to talk to us, and would even ask us to write a blog post. She also mentioned that the team would know the most about cutting-edge learnr stuff, including things like a "completion rate" bar that shows students how far they are through a tutorial.

* Any tips for remote teaching or learning? Using learnr at all is a big step. Allison also pre-records short versions of her lectures for her students to stream at a later time,  and holds smaller discussion sections in class where they do activities.

* Allison's #1 Recommendation to remote teachers of R: Having students start out with RStudio Cloud and then transitioning to Desktop. You can set up workspaces so the necessary packages are already loaded and installed, any script is already pre-loaded, and all students need to do is log in. It looks exactly the same as RStudio Desktop except folks don't have to worry about installing R or RStudio. You can push/pull from git in RStudio Cloud as well (link to a Rstudio cloud tutorial/article: https://rstudio.com/resources/webinars/teaching-r-online-with-rstudio-cloud/)

## Videos Worth Looking At?

Data Science Mini Projects: https://www.youtube.com/playlist?list=PLpdmBGJ6ELUJM6_vKQbpi9vGc65Sn0uPr
Draw Multiple Time Series in Same Plot: https://www.youtube.com/watch?v=FNXnJ_zT1gE
Barplot in R: https://www.youtube.com/watch?v=pYbuWU77QkU&feature=youtu.be

# Interestin approach for next summer

## Material from JSM 2021

Education section: https://matackett.github.io/open-stat-ds-ed/



Interesting discussion of using art to bring students into data science.

Consider the use of ggirl package for encouraging students to send someone an actual postcard.

Mine's discussion of how to make html and pdf versions of knitted R markdown look the same. Use fenced div blocks and then special CSS and LaTeX code chunks. Details: https://speakerdeck.com/minecr/openintro-building-sustaining-supporting-and-growing-open-source-educational-resources?slide=13

## Discussion of Tutorials

Let's discuss the six broad categories of tutorials which we might add, especially to chapters 5 through 11.

First, we can just do more examples. Use different data with a different question, but all the same steps as in the chapter itself. Such examples will generally, like 082, do two analyses: one with all the data (to show the truth) and one in which representativeness fails in a way which we define, thereby showing how we can be led astray. Last step is to compare the answers from these two approaches, thus emphasizing the importance of humility.

Second, stand-alone tutorials on useful topics, like text modeling or networks or rtweet or whatever. These could go in any chapter and would not be restricted to having X number of parameters.

Third, we could cover many other model families, again perhaps not worrying too much about whether or not they have the "right" number of parameters. Simplest here would be other stan_glm families. That is, show a weird data set in which the outcome variable is something that should be fit with poisson or something, and then use family = poisson. Goal would be to emphasize that there are many possible models.

Fourth, expanding on the above, we might have some machine learning examples. After all, a machine-learning model is just a different kind of black box, not different conceptually than a linear or logistic model. We could fit a machine learning model on chapter 8 scenarios. Even though there are way more than three parameters in such models, they still can (I think!) be made to work in cases with just one input variable.

Fifth, real world data science examples which do not hide the messy complexity of the world. I guess that these would be a lot like the overview tutorials, but with 30 minutes of data cleaning first. These might also explicitly assume that you have done all the other tutorials in the chapter and that, therefore, we do not need to walk you through how to estimate a `stan_glm()` with five baby questions. Instead, the questions would be, not really harder, but messier.

Sixth, tutorials about interpreting parameters. We could have one of these in every chapter from 6 onward. These could be short! They would only cover parameter interpretation, perhaps only using the models which were used in the chapter.

Obviously, we could also combine these in various ways. Upon reflection, we should save the machine learning examples for later chapters in which we have several righthand side variables. Indeed, we could have three machine learning tutorials in chapter 11.

### Later tutorials in later chapters

The first tutorial for each of the later chapters is easy. Just do more or less exactly what the chapter does. Recall that, one day, we expect to incorporate those questions directly into the chapter itself. So, the more that it just forces students to type in the same commands as in the chapter, the better. But, for the most part, the chapters are not that long! There is only about one tutorial worth of material.

What do we do for the other 5 or so tutorials for each chapter? Good question! Honestly, I am not sure. Here are some ideas:

* The second tutorial could be very similar to the first -- and, therefore, to the chapter. Just use a different variable(s) and/or a different tibble, but answer the same sorts of questions in the same way.

* Don't be repetitive in the written questions. It is fine, in the first tutorial for each chapter, to ask students to write a definition of the Preceptor Table. Indeed, we should always have that question in each first tutorial. But, we don't want that question in each of the 5 tutorials for a single chapter.

* Use written questions which do require different answers depending in the data/variable used. We can't resuse "Define a Preceptor Table" because that question has the same answer every time. Consider a different question:

> Write a paragraph which explores reasons why we should **not** consider the Preceptor Table and our data set as being drawn from the same population?

That is a good question, not least because it needs a different answer for each data science problem we confront. Another good example:

> Write a paragraph explaining why, even though the data is not a random sample, we may still consider it to be "respsentative" enough to move forward?

Again, this requires a different answer for each new tibble.

* We don't usually (ever?) ask questions about the four Cardinal Virtues directly. They are simply a pedagogical device we use to help students organize their work. But we do ask written questions about the key concepts (population, representativeness, validity) in each and every tutorial. These are difficult concepts with no "right" answers. We need to wrestle with them every week.

* A tutorial after the first might do a very similar exercise but with three differences. First, it can go faster, given that students have already seen the basics fairly slowly in the first tutorial. Second, it can add some complexity, make the modeling problem just a little more difficult than the basic case. Third, it can add some new magic. Perhaps it grabs data using a new package instead of just using another boring data set from **primer.data**. Perhaps it creates a nicer plot with some geoms we have not used before.

* I will distribute the problem sets we used last semester. Turning them into tutorials might work well. But always remember that tutorials are easy, while problem sets are hard. So, we need to split up a big problem set question into 15 exercises and then walk the students slowly (and with hints) through those 10 exercises.

* Maybe there are easy ways, in tutorial 2 or 3, to take the problem we solved in tutorial 1 and have it loosen the assumption that the data we have is representative of the population. This is a nice assumption to loosen since it is never true. I am not sure how one would do this . . .

* If the main example is the chapter is one which uses a linear model, then tutorial 2 could use a logistic model and then tutorial 3 could use the third kind of model (which we have not chosen yet). Similarly, if the chapter (and first tutorial) does a predictive mode, then tutorial 3 should do a causal model.

* The last tutorial might, in some sense, try to set the stage for the next chapter, provide a teaser for what we are learning next.

* Another type of tutorial is one which uses fake data which has been manipulated to violate the assumptions which students are making. Show them Preceptor's Posterior and compare it to their own.

* Seems like you can't include LaTeX math in a R exercise message. Generates an error under R CMD check. Example of something which fails:

Add the formula to `model.qmd`, `$$ biden_i =  \mu + \epsilon_i $$`. `Command/Ctrl + Shift + K`. Ensure that the formula is looks correct. 

<!-- # ```{r courage-12} -->
<!-- # question_text(NULL, -->
<!-- # 	message = "$$ biden_i =  \mu + \epsilon_i $$", -->
<!-- # 	answer(NULL, correct = TRUE), -->
<!-- # 	allow_retry = FALSE, -->
<!-- # 	incorrect = NULL, -->
<!-- # 	rows = 6) -->
<!-- # ``` -->
